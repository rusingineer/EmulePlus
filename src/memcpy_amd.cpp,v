head	1.30;
access;
symbols
	PublicRelease_1_2e:1.30
	Interim_Release_1-2e_RC1:1.30
	PublicRelease_1_2d:1.30
	Interim_Release_1-2d_RC1:1.30
	Interim_Release_1-2d_beta1:1.30
	PublicRelease_1_2c:1.30
	Interim_Release_1-2c_RC1:1.30
	Interim_Release_1-2c_beta1:1.30
	PublicRelease_1_2b:1.30
	Interim_Release_1-2b_RC1:1.30
	PublicRelease_1_2a:1.30
	Interim_Release_1-2a_RC1:1.30
	Interim_Release_1-2a_beta2:1.30
	Interim_Release_1-2a_beta1:1.30
	PublicRelease_1_2:1.30
	Interim_Release_1-2_RC1:1.30
	Interim_Release_1-2_beta1:1.30
	PublicRelease_1_1g:1.30
	Interim_Release_1-1g_RC3:1.30
	Interim_Release_1-1g_RC2:1.30
	Interim_Release_1-1g_RC1:1.30
	Interim_Release_1-1g_beta2:1.30
	Interim_Release_1-1g_beta1:1.30
	PublicRelease_1_1f:1.30
	Interim_Release_1-1f_RC1:1.30
	PublicRelease_1_1e:1.30
	Interim_Release_1-1e_RC2:1.30
	Interim_Release_1-1e_RC1:1.30
	Interim_Release_1-1e_beta1:1.30
	PublicRelease_1_1d:1.30
	Interim_Release_1-1d_RC1:1.30
	PublicRelease_1_1c:1.30
	Interim_Release_1-1c_RC1:1.30
	Interim_Release_1-1c_beta2:1.30
	Interim_Release_1-1c_beta1:1.30
	PublicRelease_1_1b:1.30
	Interim_Release_1-1b_RC1:1.30
	PublicRelease_1_1a:1.30
	Interim_Release_1-1a_RC2:1.30
	Interim_Release_1-1a_RC1:1.30
	Interim_Release_1-1a_beta2:1.30
	Interim_Release_1-1a_beta1:1.30
	PublicRelease_1_1:1.30
	Interim_Release_1-1_beta1:1.30
	PublicRelease_1o:1.30
	Interim_Release_1o_RC1:1.30
	Interim_Release_1o_beta1:1.30
	PublicRelease_1n:1.30
	Interim_Release_1n_RC2:1.30
	Interim_Release_1n_RC1:1.30
	Interim_Release_1n_beta2:1.30
	Interim_Release_1n_beta1:1.30
	PublicRelease_1m:1.30
	Interim_Release_1m_beta1:1.30
	PublicRelease_1l:1.30
	Interim_Release_1l_RC3:1.30
	Interim_Release_1l_RC2:1.30
	Interim_Release_1l_RC1:1.30
	Interim_Release_1l_beta2:1.30
	Interim_Release_1l_beta1:1.30
	PublicRelease_1k:1.30
	Interim_Release_1k_RC4:1.30
	Interim_1k_RC3:1.30
	Interim_1k_RC2:1.30
	Interim_Release_1k_RC1:1.30
	Interim_Release_1k_beta5:1.30
	Intrerim_Release_1k_beta4:1.30
	Interim_Release_1k_beta1:1.30
	PublicRelease_1j:1.17
	Interim_Release_1J_RC3:1.17
	Interim_Release_1j_RC3:1.17
	Interim_Release_1j_RC2:1.16
	Interim_Release_1j_RC1:1.16
	Interim_Release_1j_beta2:1.16
	Interim_Release_1j_beta1:1.15
	PublicRelease_1i:1.15
	Interim_Release_1i_RC6:1.15
	Interim_Release_1i_RC3:1.12
	Interim_Release_1i_RC2:1.12
	Interim_Release_1i_RC1:1.9
	Interim_Release_1i_beta3:1.9
	Interim_Release_1i_beta2:1.9
	Interim_Release_1i_beta1:1.1
	PublicRelease_1h:1.1
	Interim_Release_1h_rc2:1.1
	Interim_Release_1h_RC1:1.1
	Interim_Release_1h_beta2:1.1
	Interim_Release_1h_beta1_now:1.1
	Interim_Release_1h_beta1:1.1
	PublicRelease_1g:1.1
	Interim_Release_1g_RC6_Final:1.1
	Interim_Release_1g_RC6:1.1
	Interim_Release_1g_RC5:1.1
	Interim_Release_1g_RC4:1.1
	Interim_Release_1g_RC3:1.1;
locks; strict;
comment	@// @;


1.30
date	2004.02.11.23.42.20;	author aw3;	state Exp;
branches;
next	1.29;

1.29
date	2004.02.11.04.13.02;	author katsyonak;	state Exp;
branches;
next	1.28;

1.28
date	2004.02.02.16.35.48;	author katsyonak;	state Exp;
branches;
next	1.27;

1.27
date	2004.02.02.16.20.06;	author katsyonak;	state Exp;
branches;
next	1.26;

1.26
date	2004.02.01.23.29.23;	author katsyonak;	state Exp;
branches;
next	1.25;

1.25
date	2004.01.31.16.48.24;	author syrus77;	state Exp;
branches;
next	1.24;

1.24
date	2004.01.31.01.25.21;	author katsyonak;	state Exp;
branches;
next	1.23;

1.23
date	2004.01.28.22.44.46;	author katsyonak;	state Exp;
branches;
next	1.22;

1.22
date	2004.01.28.17.24.46;	author katsyonak;	state Exp;
branches;
next	1.21;

1.21
date	2004.01.28.05.31.33;	author katsyonak;	state Exp;
branches;
next	1.20;

1.20
date	2004.01.28.01.38.15;	author katsyonak;	state Exp;
branches;
next	1.19;

1.19
date	2004.01.27.16.47.36;	author katsyonak;	state Exp;
branches;
next	1.18;

1.18
date	2004.01.27.05.05.24;	author katsyonak;	state Exp;
branches;
next	1.17;

1.17
date	2004.01.14.19.55.34;	author katsyonak;	state Exp;
branches;
next	1.16;

1.16
date	2004.01.04.16.31.16;	author dongato;	state Exp;
branches;
next	1.15;

1.15
date	2003.11.28.18.44.32;	author katsyonak;	state Exp;
branches;
next	1.14;

1.14
date	2003.11.28.16.37.53;	author katsyonak;	state Exp;
branches;
next	1.13;

1.13
date	2003.11.24.15.01.39;	author katsyonak;	state Exp;
branches;
next	1.12;

1.12
date	2003.11.10.14.29.13;	author katsyonak;	state Exp;
branches;
next	1.11;

1.11
date	2003.11.10.12.47.04;	author katsyonak;	state Exp;
branches;
next	1.10;

1.10
date	2003.11.05.15.32.43;	author katsyonak;	state Exp;
branches;
next	1.9;

1.9
date	2003.10.22.02.58.26;	author katsyonak;	state Exp;
branches;
next	1.8;

1.8
date	2003.10.22.00.39.04;	author katsyonak;	state Exp;
branches;
next	1.7;

1.7
date	2003.10.22.00.03.00;	author katsyonak;	state Exp;
branches;
next	1.6;

1.6
date	2003.10.15.11.45.03;	author syrus77;	state Exp;
branches;
next	1.5;

1.5
date	2003.10.14.18.40.58;	author syrus77;	state Exp;
branches;
next	1.4;

1.4
date	2003.10.12.22.39.04;	author dongato;	state Exp;
branches;
next	1.3;

1.3
date	2003.10.09.15.12.57;	author dongato;	state Exp;
branches;
next	1.2;

1.2
date	2003.10.08.12.56.33;	author morevit;	state Exp;
branches;
next	1.1;

1.1
date	2003.06.11.18.46.26;	author eklmn;	state Exp;
branches;
next	;


desc
@@


1.30
log
@memzero optimization -- no return value, unified reset
@
text
@/******************************************************************************

 Copyright (c) 2001 Advanced Micro Devices, Inc.

 LIMITATION OF LIABILITY:  THE MATERIALS ARE PROVIDED *AS IS* WITHOUT ANY
 EXPRESS OR IMPLIED WARRANTY OF ANY KIND INCLUDING WARRANTIES OF MERCHANTABILITY,
 NONINFRINGEMENT OF THIRD-PARTY INTELLECTUAL PROPERTY, OR FITNESS FOR ANY
 PARTICULAR PURPOSE.  IN NO EVENT SHALL AMD OR ITS SUPPLIERS BE LIABLE FOR ANY
 DAMAGES WHATSOEVER (INCLUDING, WITHOUT LIMITATION, DAMAGES FOR LOSS OF PROFITS,
 BUSINESS INTERRUPTION, LOSS OF INFORMATION) ARISING OUT OF THE USE OF OR
 INABILITY TO USE THE MATERIALS, EVEN IF AMD HAS BEEN ADVISED OF THE POSSIBILITY
 OF SUCH DAMAGES.  BECAUSE SOME JURISDICTIONS PROHIBIT THE EXCLUSION OR LIMITATION
 OF LIABILITY FOR CONSEQUENTIAL OR INCIDENTAL DAMAGES, THE ABOVE LIMITATION MAY
 NOT APPLY TO YOU.

 AMD does not assume any responsibility for any errors which may appear in the
 Materials nor any responsibility to support or update the Materials.  AMD retains
 the right to make changes to its test specifications at any time, without notice.

 NO SUPPORT OBLIGATION: AMD is not obligated to furnish, support, or make any
 further information, software, technical information, know-how, or show-how
 available to you.

 So that all may benefit from your experience, please report  any  problems
 or  suggestions about this software to 3dsdk.support@@amd.com

 AMD Developer Technologies, M/S 585
 Advanced Micro Devices, Inc.
 5900 E. Ben White Blvd.
 Austin, TX 78741
 3dsdk.support@@amd.com
******************************************************************************/
#pragma once

#include "stdafx.h"
#include "memcpy_amd.h"

/*****************************************************************************
MEMCPY_AMD.CPP
******************************************************************************/

// Very optimized memcpy() routine for all AMD Athlon and Duron family.
// This code uses any of FOUR different basic copy methods, depending
// on the transfer size.
// NOTE:  Since this code uses MOVNTQ (also known as "Non-Temporal MOV" or
// "Streaming Store"), and also uses the software prefetchnta instructions,
// be sure you're running on Athlon/Duron or other recent CPU before calling!

#define IN_CACHE_COPY 64 * 1024  // upper limit for movq/movq copy w/SW prefetch
// Next is a copy that uses the MMX registers to copy 8 bytes at a time,
// also using the "unrolled loop" optimization.   This code uses
// the software prefetch instruction to get the data into the cache.

#define UNCACHED_COPY 197 * 1024 // upper limit for movq/movntq w/SW prefetch
// For larger blocks, which will spill beyond the cache, it's faster to
// use the Streaming Store instruction MOVNTQ.   This write instruction
// bypasses the cache and writes straight to main memory.  This code also
// uses the software prefetch instruction to pre-read the data.
// USE 64 * 1024 FOR THIS VALUE IF YOU'RE ALWAYS FILLING A "CLEAN CACHE"

#define BLOCK_PREFETCH_COPY  infinity // no limit for movq/movntq w/block prefetch 
#define CACHEBLOCK 80h // number of 64-byte blocks (cache lines) for block prefetch
// For the largest size blocks, a special technique called Block Prefetch
// can be used to accelerate the read operations.   Block Prefetch reads
// one address per cache line, for a series of cache lines, in a short loop.
// This is faster than using software prefetch.  The technique is great for
// getting maximum read bandwidth, especially in DDR memory systems.

// Inline assembly syntax for use with Visual C++

/////////////////////////////////////////////////////////////////////////////////////
// katsyonak: Added MMX & SSE optimized memcpy - October 8, 2003				  //
//																				 //
// katsyonak: Added AMD, MMX & SSE optimized memset - October 12, 2003			//
//														    				   //
// Aw3/katsyonak: Added AMD, MMX & SSE optimized memzero - February 11, 2004  //
///////////////////////////////////////////////////////////////////////////////

static unsigned long CPU_Type = 0;
// 0 = CPU check not performed yet (Auto detect)
// 1 = No optimization
// 2 = MMX
// 3 = MMX2 for AMD Athlon/Duron and above (might also work on MMX2 (KATMAI) Intel machines)
// 4 = SSE
// 5 = SSE2 (only for Pentium 4 detection, the optimization used is SSE)
unsigned long get_cpu_type()
{
  __asm
  {
	mov			eax, [CPU_Type]
	cmp			eax, 5
	ja			do_detect
	or			eax, eax
	jne			ret_eax
do_detect:
	xor			eax, eax
	cpuid
	or			eax, eax
	mov			eax, 1 ;No optimization
	je			cpu_done
	xor			esi, esi
	cmp			ebx, 68747541h ;Auth
	jne			not_amd
	cmp			edx, 69746E65h ;enti
	jne			not_amd
	cmp			ecx, 444D4163h ;cAMD
	jne			not_amd
	inc			esi
not_amd:
	;mov			eax,1
	cpuid
	mov			al, 1 ;No optimization
	bt			edx, 23 ;MMX Feature Bit
	jnb			ret_al
	or			esi, esi
	je			check_sse
	and			ah, 1111b
	cmp			ah, 6 ;model 6 (K7) = Athlon, Duron
	jb			cpu_mmx
	mov			eax, 80000000h
	cpuid
	cmp			eax, 80000000h
	jbe			cpu_mmx
	mov			eax, 80000001h
	cpuid
	bt			edx, 31 ;AMD Feature Bit
	jnb			cpu_mmx
	mov			al, 3 ;AMD
	jmp			ret_al
check_sse:
	bt			edx, 25 ;SSE Feature Bit
	jb			cpu_sse
cpu_mmx:
	mov			al, 2
	jmp			ret_al
cpu_sse:
	mov			al, 4 ;SSE
	bt			edx, 26 ;SSE2 Feature Bit
	adc			al, 0
ret_al:
	movzx		eax,al
cpu_done:
	mov			[CPU_Type], eax
ret_eax:
  }
}

static unsigned long memcpyProc = 0;
static unsigned long memsetProc = 0;
static unsigned long memzeroProc = 0;

void* __stdcall memcpy_optimized(void *dest, const void *src, size_t n)
{
  __asm
  {
	mov			ebx, [n]		; number of bytes to copy
	mov			edi, [dest]		; destination
	mov			esi, [src]		; source
	push		edi

	mov			ecx, [memcpyProc]
	jecxz		$memcpy_detect
	jmp			ecx

$memcpy_detect:
	push		ebx
	push		esi
	push		edi
	call		get_cpu_type
	mov			ecx, offset copy_sse
	cmp			al, 3
	ja			addr_done
	mov			ecx, offset copy_amd
	je			addr_done
	mov			ecx, offset copy_mmx
	cmp			al, 1
	ja			addr_done
	mov			ecx, offset copy_rep
addr_done:
	mov			[memcpyProc], ecx
	pop			edi
	pop			esi
	pop			ebx
	jmp			ecx

align 16
copy_sse:
	cmp			ebx, 512
	jb			copy_mmx		; tiny? skip optimized copy

	mov			ecx, 16			; a trick that's faster than rep movsb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 1111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_sse_align_done
	jmp			ecx				; jump to array of movsb's

align 16
$memcpy_sse_ic_1_a:				; 64-byte block copies, in-cache copy
	prefetchnta	[esi + 320]		; start reading ahead

	movaps		xmm0, [esi]			; read 128 bits
	movaps		xmm1, [esi+16]
	movaps		xmm2, [esi+32]
	movaps		xmm3, [esi+48]
	add			esi, 64				; update source pointer
	movntps		[edi], xmm0			; write 128 bits
	movntps		[edi+16], xmm1
	movntps		[edi+32], xmm2
	movntps		[edi+48], xmm3
	add			edi, 64				; update destination pointer
	dec			ecx					; count down
	jnz			$memcpy_sse_ic_1_a	; last 64-byte block?
	sfence						; flush the write buffer
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

align 4
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb

$memcpy_sse_align_done:		; destination is double quadword aligned
	mov			ecx, ebx		; number of bytes left to copy
	shr			ecx, 6			; get 64-byte block count
	test		esi, 1111b		; Is the source address aligned?
	je			$memcpy_sse_ic_1_a

// This is small block copy that uses the SSE registers to copy 16 bytes
// at a time.  It uses the "unrolled loop" optimization, and also uses
// the software prefetch instruction to get the data into the cache.
align 16
$memcpy_sse_ic_1:			; 64-byte block copies, in-cache copy
	prefetchnta	[esi + 320]		; start reading ahead

	movups		xmm0, [esi]			; read 128 bits
	movups		xmm1, [esi+16]
	movups		xmm2, [esi+32]
	movups		xmm3, [esi+48]
	add			esi, 64				; update source pointer
	movntps		[edi], xmm0			; write 128 bits
	movntps		[edi+16], xmm1
	movntps		[edi+32], xmm2
	movntps		[edi+48], xmm3
	add			edi, 64				; update destination pointer
	dec			ecx					; count down
	jnz			$memcpy_sse_ic_1	; last 64-byte block?
	sfence						; flush the write buffer
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

align 16
copy_amd:
	cmp			ebx, 128
	jb			copy_rep		; tiny? skip optimized copy
	cmp			ebx, 32*1024			; don't align between 32k-64k because
	jbe			$memcpy_amd_do_align	;  it appears to be slower
	cmp			ebx, 64*1024
	jbe			$memcpy_amd_align_done
$memcpy_amd_do_align:
	mov			ecx, 8			; a trick that's faster than rep movsb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_amd_align_done
	jmp			ecx				; jump to array of movsb's

$memcpy_amd_uc_test:
	cmp			ecx, UNCACHED_COPY/64	; big enough? use block prefetch copy
	jae			$memcpy_amd_bp_1

// For larger blocks, which will spill beyond the cache, it's faster to
// use the Streaming Store instruction MOVNTQ.   This write instruction
// bypasses the cache and writes straight to main memory.  This code also
// uses the software prefetch instruction to pre-read the data.
align 16
$memcpy_amd_uc_1:				; 64-byte blocks, uncached copy
	prefetchnta	[esi + (200*64/34+192)]		; start reading ahead

	movq		mm0, [esi]		; read 64 bits
	add			edi, 64			; update destination pointer
	movq		mm1, [esi+8]
	add			esi, 64			; update source pointer
	movq		mm2, [esi-48]
	movntq		[edi-64], mm0	; write 64 bits, bypassing the cache
	movq		mm0, [esi-40]	;    note: movntq also prevents the CPU
	movntq		[edi-56], mm1	;    from READING the destination address
	movq		mm1, [esi-32]	;    into the cache, only to be over-written
	movntq		[edi-48], mm2	;    so that also helps performance
	movq		mm2, [esi-24]
	movntq		[edi-40], mm0
	movq		mm0, [esi-16]
	movntq		[edi-32], mm1
	movq		mm1, [esi-8]
	movntq		[edi-24], mm2
	movntq		[edi-16], mm0
	dec			ecx
	movntq		[edi-8], mm1
	jnz			$memcpy_amd_uc_1	; last 64-byte block?
	sfence						; flush the write buffer
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

// For the largest size blocks, a special technique called Block Prefetch
// can be used to accelerate the read operations.   Block Prefetch reads
// one address per cache line, for a series of cache lines, in a short loop.
// This is faster than using software prefetch, in this case.
// The technique is great for getting maximum read bandwidth,
// especially in DDR memory systems.
$memcpy_amd_bp_1:				; large blocks, block prefetch copy
	mov			eax, CACHEBLOCK / 2		; block prefetch loop, unrolled 2X
	add			esi, CACHEBLOCK * 64	; move to the top of the block
align 16
$memcpy_amd_bp_2:
	mov			edx, [esi-64]		; grab one address per cache line
	mov			edx, [esi-128]		; grab one address per cache line
	sub			esi, 128			; go reverse order
	dec			eax					; count down the cache lines
	jnz			$memcpy_amd_bp_2	; keep grabbing more lines into cache

	mov			eax, CACHEBLOCK		; now that it's in cache, do the copy
align 16
$memcpy_amd_bp_3:
	movq		mm0, [esi]			; read 64 bits
	movq		mm1, [esi+8]
	movq		mm2, [esi+16]
	movq		mm3, [esi+24]
	movq		mm4, [esi+32]
	movq		mm5, [esi+40]
	movq		mm6, [esi+48]
	movq		mm7, [esi+56]
	add			esi, 64				; update source pointer
	movntq		[edi], mm0			; write 64 bits, bypassing cache
	movntq		[edi+8], mm1		;    note: movntq also prevents the CPU
	movntq		[edi+16], mm2		;    from READING the destination address 
	movntq		[edi+24], mm3		;    into the cache, only to be over-written,
	movntq		[edi+32], mm4		;    so that also helps performance
	movntq		[edi+40], mm5
	movntq		[edi+48], mm6
	movntq		[edi+56], mm7
	add			edi, 64				; update dest pointer
	dec			eax					; count down
	jnz			$memcpy_amd_bp_3	; keep copying
	sub			ecx, CACHEBLOCK		; update the 64-byte block count
	jbe		$memcpy_done				; no more 64-byte blocks left
	cmp			ecx, CACHEBLOCK			; big enough to run another prefetch loop?
	jae			$memcpy_amd_bp_1		; yes, keep processing chunks
	jmp			$memcpy_amd_uc_1		; 64-byte blocks, uncached copy

align 4
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb

$memcpy_amd_align_done:			; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to copy
	shr			ecx, 6			; get 64-byte block count
	cmp			ecx, IN_CACHE_COPY/64	; too big 4 cache? use uncached copy
	jae			$memcpy_amd_uc_test

// This is small block copy that uses the MMX registers to copy 8 bytes
// at a time.  It uses the "unrolled loop" optimization, and also uses
// the software prefetch instruction to get the data into the cache.
align 16
$memcpy_amd_ic_1:				; 64-byte block copies, in-cache copy
	prefetchnta	[esi + (200*64/34+192)]		; start reading ahead

	movq		mm0, [esi]		; read 64 bits
	movq		mm1, [esi+8]
	movq		[edi], mm0		; write 64 bits
	movq		[edi+8], mm1	;    note:  the normal movq writes the
	movq		mm2, [esi+16]	;    data to cache; a cache line will be
	movq		mm3, [esi+24]	;    allocated as needed, to store the data
	movq		[edi+16], mm2
	movq		[edi+24], mm3
	movq		mm0, [esi+32]
	movq		mm1, [esi+40]
	movq		[edi+32], mm0
	movq		[edi+40], mm1
	movq		mm2, [esi+48]
	movq		mm3, [esi+56]
	movq		[edi+48], mm2
	movq		[edi+56], mm3

	add			esi, 64			; update source pointer
	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memcpy_amd_ic_1	; last 64-byte block?

$memcpy_done:
	sfence						; flush the write buffer
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

align 16
copy_mmx:
	cmp			ebx, 128
	jb			copy_rep		; tiny? skip optimized copy

	mov			ecx, 8			; a trick that's faster than rep movsb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_mmx_align_done
	jmp			ecx				; jump to array of movsb's

align 4
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb

$memcpy_mmx_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to copy
	shr			ecx, 6			; get 64-byte block count

align 16
$memcpy_mmx_ic_1:
	movq		mm0, [esi]		; read 64 bits
	movq		mm1, [esi+8]
	movq		[edi], mm0		; write 64 bits
	movq		[edi+8], mm1
	movq		mm2, [esi+16]
	movq		mm3, [esi+24]
	movq		[edi+16], mm2
	movq		[edi+24], mm3
	movq		mm0, [esi+32]
	movq		mm1, [esi+40]
	movq		[edi+32], mm0
	movq		[edi+40], mm1
	movq		mm2, [esi+48]
	movq		mm3, [esi+56]
	movq		[edi+48], mm2
	movq		[edi+56], mm3

	add			esi, 64			; update source pointer
	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memcpy_mmx_ic_1	; last 64-byte block?
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

align 16
copy_rep:
	mov			ecx, ebx
	shr			ecx, 2
	and			ebx, 11b		; ebx isn't required any more
	rep			movsd
	mov			ecx, ebx
	rep			movsb
	jmp			$memcpy_exit

// The smallest copy uses the X86 "movsd" instruction, in an optimized
// form which is an "unrolled loop".   Then it handles the last few bytes.
align 4
	movsd
	movsd			; perform last 1-15 dword copies
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd			; perform last 1-7 dword copies
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd

$memcpy_last_few:			; dword aligned from before movsd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			movsb		; the last 1, 2, or 3 bytes
	emms

$memcpy_exit:
	pop			eax // [dest]	; ret value = destination pointer
    }
}

void* __stdcall memset_optimized(void *dest, int c, size_t n)
{
  __asm
  {
	mov			ebx, [n]	; number of bytes to fill
	mov			edi, [dest]	; destination
	movzx  		eax, [c]	; character
	mov			ah, al
	mov			ecx, eax
	shl			ecx, 16
	push		edi
	or			eax, ecx

	mov			ecx,[memsetProc]
	jecxz		$memset_detect
	jmp			ecx

$memset_detect:
	push		eax
	push		ebx
	push		edi
	call		get_cpu_type
	mov			ecx, offset fill_sse
	cmp			al, 3
	ja			addr_done
	mov			ecx, offset fill_amd
	je			addr_done
	mov			ecx, offset fill_mmx
	cmp			al, 1
	ja			addr_done
	mov			ecx, offset fill_rep
addr_done:
	mov			[memsetProc], ecx
	pop			edi
	pop			ebx
	pop			eax
	jmp			ecx

align 16
fill_sse:
	cmp			ebx, 2048
	jb			fill_mmx		; tiny? skip optimized fill

	mov			ecx, 16			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 1111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_sse_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memset_sse_align_done:		; destination is double quadword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	push		eax
	push		eax
	push		eax
	push		eax
	movups		xmm0, [esp]
	add			esp, 16

align 16
$memset_sse_ic_1:
	movntps		[edi], xmm0			; write 128 bits
	movntps		[edi+16], xmm0
	movntps		[edi+32], xmm0
	movntps		[edi+48], xmm0

	add			edi, 64				; update destination pointer
	dec			ecx					; count down
	jnz			$memset_sse_ic_1	; last 64-byte block?
	sfence						; flush the write buffer
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_last_few
	jmp			ecx				; jump to array of stosd's

align 16
fill_amd:
	cmp			ebx, 128
	jb			fill_rep		; tiny? skip optimized fill

	mov			ecx, 8			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update fill count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_amd_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memset_amd_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	movd		mm0, eax
	punpckldq	mm0, mm0

align 16
$memset_amd_ic_1:
	movntq		[edi], mm0		; write 64 bits
	movntq		[edi+8], mm0
	movntq		[edi+16], mm0
	movntq		[edi+24], mm0
	movntq		[edi+32], mm0
	movntq		[edi+40], mm0
	movntq		[edi+48], mm0
	movntq		[edi+56], mm0

	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memset_amd_ic_1	; last 64-byte block?
	sfence						; flush the write buffer
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_last_few
	jmp			ecx				; jump to array of stosd's

align 16
fill_mmx:
	cmp			ebx, 192
	jb			fill_rep		; tiny? skip optimized fill

	mov			ecx, 8			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update fill count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_mmx_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memset_mmx_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	movd		mm0, eax
	punpckldq	mm0, mm0

align 16
$memset_mmx_ic_1:
	movq		[edi], mm0		; write 64 bits
	movq		[edi+8], mm0
	movq		[edi+16], mm0
	movq		[edi+24], mm0
	movq		[edi+32], mm0
	movq		[edi+40], mm0
	movq		[edi+48], mm0
	movq		[edi+56], mm0

	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memset_mmx_ic_1	; last 64-byte block?
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_last_few
	jmp			ecx				; jump to array of stosd's

align 16
fill_rep:
	mov			ecx, ebx
	shr			ecx, 2
	and			ebx, 11b		; ebx isn't required any more
	rep			stosd
	mov			ecx, ebx
	rep			stosb
	jmp			$memset_exit

align 4
	stosd
	stosd			; perform last 1-15 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd			; perform last 1-7 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd

$memset_last_few:		; dword aligned from before stosd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			stosb		; the last 1, 2, or 3 bytes
	emms

$memset_exit:
	pop			eax // [dest]	; ret value = destination pointer
    }
}

void __stdcall memzero_optimized(void *dest, size_t n)
{
  __asm
  {
	mov			ebx, [n]	; number of bytes to fill
	mov			edi, [dest]	; destination
	xor			eax, eax

	mov			ecx,[memzeroProc]
	jecxz		$memzero_detect
	jmp			ecx

$memzero_detect:
	push		ebx
	push		edi
	call		get_cpu_type
	mov			ecx, offset fill_sse
	cmp			al, 3
	ja			addr_done
	mov			ecx, offset fill_amd
	je			addr_done
	mov			ecx, offset fill_mmx
	cmp			al, 1
	ja			addr_done
	mov			ecx, offset fill_rep
addr_done:
	mov			[memzeroProc], ecx
	pop			edi
	pop			ebx
	xor			eax, eax
	jmp			ecx

align 16
fill_sse:
	cmp			ebx, 2048
	jb			fill_mmx		; tiny? skip optimized fill

	mov			ecx, 16			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 1111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memzero_sse_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memzero_sse_align_done:		; destination is double quadword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	xorps		xmm0, xmm0

align 16
$memzero_sse_ic_1:
	movntps		[edi], xmm0			; write 128 bits
	movntps		[edi+16], xmm0
	movntps		[edi+32], xmm0
	movntps		[edi+48], xmm0
	add			edi, 64				; update destination pointer
	dec			ecx					; count down
	jnz			$memzero_sse_ic_1	; last 64-byte block?
	sfence						; flush the write buffer
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memzero_last_few
	jmp			ecx				; jump to array of stosd's

align 16
fill_amd:
	cmp			ebx, 128
	jb			fill_rep		; tiny? skip optimized fill

	mov			ecx, 8			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update fill count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memzero_amd_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memzero_amd_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	pxor		mm0, mm0

align 16
$memzero_amd_ic_1:
	movntq		[edi], mm0		; write 64 bits
	movntq		[edi+8], mm0
	movntq		[edi+16], mm0
	movntq		[edi+24], mm0
	movntq		[edi+32], mm0
	movntq		[edi+40], mm0
	movntq		[edi+48], mm0
	movntq		[edi+56], mm0
	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memzero_amd_ic_1	; last 64-byte block?
	sfence						; flush the write buffer
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memzero_last_few
	jmp			ecx				; jump to array of stosd's

align 16
fill_mmx:
	cmp			ebx, 192
	jb			fill_rep		; tiny? skip optimized fill

	mov			ecx, 8			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update fill count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memzero_mmx_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memzero_mmx_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	pxor		mm0, mm0

align 16
$memzero_mmx_ic_1:
	movq		[edi], mm0		; write 64 bits
	movq		[edi+8], mm0
	movq		[edi+16], mm0
	movq		[edi+24], mm0
	movq		[edi+32], mm0
	movq		[edi+40], mm0
	movq		[edi+48], mm0
	movq		[edi+56], mm0
	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memzero_mmx_ic_1	; last 64-byte block?
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memzero_last_few
	jmp			ecx				; jump to array of stosd's

align 16
fill_rep:
	mov			ecx, ebx
	shr			ecx, 2
	and			ebx, 11b		; ebx isn't required any more
	rep			stosd
	mov			ecx, ebx
	rep			stosb
	jmp			$memzero_exit

align 4
	stosd
	stosd			; perform last 1-15 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd			; perform last 1-7 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd

$memzero_last_few:		; dword aligned from before stosd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			stosb		; the last 1, 2, or 3 bytes
	emms

$memzero_exit:
    }
}
@


1.29
log
@Replaced all zero filled memsets with optimized memzero (Thx Aw3 !) + some more Aw3 optimizations
@
text
@d766 1
a766 1
void* __stdcall memzero_optimized(void *dest, size_t n)
d772 1
a772 1
	push		edi
d795 1
a802 1
	xor			eax, eax
a855 1
	xor			eax, eax
a904 1
	xor			eax, eax
a949 1
	xor			eax, eax
a982 1
	pop			eax // [dest]	; ret value = destination pointer
@


1.28
log
@small change and a fix
@
text
@d71 7
a77 5
/////////////////////////////////////////////////////////////////////////////
// katsyonak: Added MMX & SSE optimized memcpy - October 8, 2003		  //
//																		 //
// katsyonak: Added AMD, MMX & SSE optimized memset - October 12, 2003  //
///////////////////////////////////////////////////////////////////////
d79 1
a79 1
unsigned long CPU_Type = 0;
d148 3
a150 2
unsigned long memcpyProc = 0;
unsigned long memsetProc = 0;
d204 4
a208 1
	movaps		xmm1, [esi+16]
a209 1
	movaps		xmm2, [esi+32]
a210 1
	movaps		xmm3, [esi+48]
a211 2

	add			esi, 64				; update source pointer
d255 4
a259 1
	movups		xmm1, [esi+16]
a260 1
	movups		xmm2, [esi+32]
a261 1
	movups		xmm3, [esi+48]
a262 2

	add			esi, 64				; update source pointer
a265 2

$memcpy_done:
a294 3
$memcpy_amd_64_test:			; tail end of block prefetch will jump here
	jecxz		$memcpy_done	; no more 64-byte blocks left

d331 46
d421 2
a430 46
// For the largest size blocks, a special technique called Block Prefetch
// can be used to accelerate the read operations.   Block Prefetch reads
// one address per cache line, for a series of cache lines, in a short loop.
// This is faster than using software prefetch, in this case.
// The technique is great for getting maximum read bandwidth,
// especially in DDR memory systems.
$memcpy_amd_bp_1:				; large blocks, block prefetch copy
	cmp			ecx, CACHEBLOCK			; big enough to run another prefetch loop?
	jl			$memcpy_amd_64_test		; no, back to regular uncached copy

	mov			eax, CACHEBLOCK / 2		; block prefetch loop, unrolled 2X
	add			esi, CACHEBLOCK * 64	; move to the top of the block
align 16
$memcpy_amd_bp_2:
	mov			edx, [esi-64]		; grab one address per cache line
	mov			edx, [esi-128]		; grab one address per cache line
	sub			esi, 128			; go reverse order
	dec			eax					; count down the cache lines
	jnz			$memcpy_amd_bp_2	; keep grabbing more lines into cache

	mov			eax, CACHEBLOCK		; now that it's in cache, do the copy
align 16
$memcpy_amd_bp_3:
	movq		mm0, [esi]			; read 64 bits
	movq		mm1, [esi+8]
	movq		mm2, [esi+16]
	movq		mm3, [esi+24]
	movq		mm4, [esi+32]
	movq		mm5, [esi+40]
	movq		mm6, [esi+48]
	movq		mm7, [esi+56]
	add			esi, 64				; update source pointer
	movntq		[edi], mm0			; write 64 bits, bypassing cache
	movntq		[edi+8], mm1		;    note: movntq also prevents the CPU
	movntq		[edi+16], mm2		;    from READING the destination address 
	movntq		[edi+24], mm3		;    into the cache, only to be over-written,
	movntq		[edi+32], mm4		;    so that also helps performance
	movntq		[edi+40], mm5
	movntq		[edi+48], mm6
	movntq		[edi+56], mm7
	add			edi, 64				; update dest pointer
	dec			eax					; count down
	jnz			$memcpy_amd_bp_3	; keep copying
	sub			ecx, CACHEBLOCK		; update the 64-byte block count
	jmp			$memcpy_amd_bp_1	; keep processing chunks

d539 1
a540 1
	push		edi
d765 224
@


1.27
log
@minor change
@
text
@a48 4
#define TINY_BLOCK_COPY 64       // upper limit for movsd type copy
// The smallest copy uses the X86 "movsd" instruction, in an optimized
// form which is an "unrolled loop".

a241 2
	;jz			$memcpy_ic_2	; finish the last few bytes

d266 1
a266 1
$memcpy_flush:
a267 1
$memcpy_ic_2:
d277 2
a278 2
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memcpy_ic_2	; tiny? skip optimized copy
d297 1
a297 1
	jecxz		$memcpy_flush	; no more 64-byte blocks left
a347 2
	;jz			$memcpy_ic_2	; finish the last few bytes

a458 1
	;jz			$memcpy_ic_2	; finish the last few bytes
a602 1
	;jz			$memset_ic_2	; finish the last few bytes
a620 2

$memset_ic_2:
d631 1
a631 1
	jb			$memset_ic_2	; tiny? skip optimized fill
a653 1
	;jz			$memset_ic_2	; finish the last few bytes
a704 1
	;jz			$memset_ic_2	; finish the last few bytes
@


1.26
log
@Some more changes (Thx Aw3!)
@
text
@d92 2
a93 2
	mov			eax,[CPU_Type]
	cmp			eax,5
d95 1
a95 1
	or			eax,eax
d98 1
a98 1
	xor			eax,eax
d100 2
a101 2
	or			eax,eax
	mov			eax,1 ;No optimization
d103 2
a104 2
	xor			esi,esi
	cmp			ebx,68747541h ;Auth
d106 1
a106 1
	cmp			edx,69746E65h ;enti
d108 1
a108 1
	cmp			ecx,444D4163h ;cAMD
d114 2
a115 2
	mov			al,1 ;No optimization
	bt			edx,23 ;MMX Feature Bit
d117 1
a117 1
	or			esi,esi
d119 2
a120 2
	and			ah,1111b
	cmp			ah,6 ;model 6 (K7) = Athlon, Duron
d122 1
a122 1
	mov			eax,80000000h
d124 1
a124 1
	cmp			eax,80000000h
d126 1
a126 1
	mov			eax,80000001h
d128 1
a128 1
	bt			edx,31 ;AMD Feature Bit
d130 1
a130 1
	mov			al,3 ;AMD
d133 1
a133 1
	bt			edx,25 ;SSE Feature Bit
d136 1
a136 1
	mov			al,2
d139 3
a141 3
	mov			al,4 ;SSE
	bt			edx,26 ;SSE2 Feature Bit
	adc			al,0
d145 1
a145 1
	mov			[CPU_Type],eax
d162 1
a162 1
	mov			ecx,[memcpyProc]
d171 2
a172 2
	mov			ecx,offset copy_sse
	cmp			al,3
d174 1
a174 1
	mov			ecx,offset copy_amd
d176 2
a177 2
	mov			ecx,offset copy_mmx
	cmp			al,1
d179 1
a179 1
	mov			ecx,offset copy_rep
d181 1
a181 1
	mov			[memcpyProc],ecx
d204 2
a205 2
	movaps		xmm0, [esi+0]		; read 128 bits
	movntps		[edi+0], xmm0		; write 128 bits
d246 1
a246 1
	jz			$memcpy_ic_2	; finish the last few bytes
d248 1
a248 1
	test		esi,1111b		; Is the source address aligned?
d258 2
a259 2
	movups		xmm0, [esi+0]		; read 128 bits
	movntps		[edi+0], xmm0		; write 128 bits
d314 5
a318 5
	movq		mm0,[esi+0]		; read 64 bits
	add			edi,64			; update destination pointer
	movq		mm1,[esi+8]
	add			esi,64			; update source pointer
	movq		mm2,[esi-48]
d320 1
a320 1
	movq		mm0,[esi-40]	;    note: movntq also prevents the CPU
d322 1
a322 1
	movq		mm1,[esi-32]	;    into the cache, only to be over-written
d324 1
a324 1
	movq		mm2,[esi-24]
d326 1
a326 1
	movq		mm0,[esi-16]
d328 1
a328 1
	movq		mm1,[esi-8]
d352 1
a352 1
$memcpy_amd_align_done:		; destination is dword aligned
d355 1
a355 1
	jz			$memcpy_ic_2	; finish the last few bytes
d364 1
a364 1
$memcpy_amd_ic_1:			; 64-byte block copies, in-cache copy
d367 1
a367 1
	movq		mm0, [esi+0]	; read 64 bits
d369 1
a369 1
	movq		[edi+0], mm0	; write 64 bits
d402 1
a402 2
$memcpy_amd_bp_1:			; large blocks, block prefetch copy

d419 2
a420 2
	movq		mm0, [esi   ]		; read 64 bits
	movq		mm1, [esi+ 8]
d428 2
a429 2
	movntq		[edi   ], mm0		; write 64 bits, bypassing cache
	movntq		[edi+ 8], mm1		;    note: movntq also prevents the CPU
d468 1
a468 1
	jz			$memcpy_ic_2	; finish the last few bytes
d472 1
a472 1
	movq		mm0, [esi+0]	; read 64 bits
d474 1
a474 1
	movq		[edi+0], mm0	; write 64 bits
d502 3
a504 3
	mov			ecx,ebx
	shr			ecx,2
	and			ebx,11b ; ebx isn't required any more
d506 1
a506 1
	mov			ecx,ebx
d563 2
a564 2
	mov			ecx,offset fill_sse
	cmp			al,3
d566 1
a566 1
	mov			ecx,offset fill_amd
d568 2
a569 2
	mov			ecx,offset fill_mmx
	cmp			al,1
d571 1
a571 1
	mov			ecx,offset fill_rep
d573 1
a573 1
	mov			[memsetProc],ecx
d613 1
a613 1
	jz			$memset_ic_2	; finish the last few bytes
d618 1
a618 1
	movups		xmm0,[esp]
d623 1
a623 1
	movntps		[edi+ 0], xmm0		; write 128 bits
d643 1
a643 1
	cmp			ebx, TINY_BLOCK_COPY
d667 1
a667 1
	jz			$memset_ic_2	; finish the last few bytes
d673 2
a674 3

	movntq		[edi+ 0], mm0	; write 64 bits
	movntq		[edi+ 8], mm0
d719 1
a719 1
	jz			$memset_ic_2	; finish the last few bytes
d725 2
a726 2
	movq		[edi+ 0], mm0	; write 64 bits
	movq		[edi+ 8], mm0
d746 3
a748 3
	mov			ecx,ebx
	shr			ecx,2
	and			ebx,11b ; ebx isn't required any more
d750 1
a750 1
	mov			ecx,ebx
@


1.25
log
@crashfix for mmx-cpu
@
text
@a161 3
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memcpy_ic_2	; tiny? skip optimized copy

d188 3
a190 8
copy_rep:
	mov			ecx,ebx	
	shr			ecx,2
	rep			movsd
	mov			ecx,ebx
	and			ecx,11b
	rep			movsb
	jmp			$memcpy_exit
a191 2
align 16
copy_sse:
d217 1
d272 2
d284 2
d304 1
a304 1
	jecxz		$memcpy_ic_2	; no more 64-byte blocks left
d334 1
d388 1
d445 3
d501 10
d535 1
a536 5
	cmp			[CPU_Type],2
	jb			$memcpy_exit
	emms				; clean up the MMX state
	je			$memcpy_exit
	sfence				; flush the write buffer
a554 3
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memset_ic_2	; tiny? skip optimized fill

d581 3
a583 8
fill_rep:
	mov			ecx,ebx
	shr			ecx,2
	rep			stosd
	mov			ecx,ebx
	and			ecx,11b
	rep			stosb
	jmp			$memset_exit
a584 2
align 16
fill_sse:
d620 1
a620 1
	add			esp,16
d632 1
d644 3
d669 2
a670 4
	push		eax
	push		eax
	movq		mm0,[esp]
	add			esp,8
d687 1
d697 3
d722 2
a723 4
	push		eax
	push		eax
	movq		mm0,[esp]
	add			esp,8
d746 10
d778 1
a779 5
	cmp			[CPU_Type],2
	jb			$memset_exit
	emms				; clean up the MMX state
	je			$memset_exit
	sfence				; flush the write buffer
@


1.24
log
@Some Aw3 changes + Reverted a change
@
text
@d526 2
a528 1
	cmp			[CPU_Type],2
d769 2
a771 1
	cmp			[CPU_Type],2
@


1.23
log
@Minor comment correction by Aw3
@
text
@a156 1
	cld
d526 1
a527 2
	jb			$memcpy_exit
	emms				; clean up the MMX state
a538 1
	cld
d618 6
a623 3
	movd		xmm0, eax
	punpckldq	xmm0, xmm0
	punpckldq	xmm0, xmm0
d668 4
a671 2
	movd		mm0, eax
	punpckldq	mm0, mm0
d719 4
a722 2
	movd		mm0, eax
	punpckldq	mm0, mm0
d768 1
a769 2
	jb			$memset_exit
	emms				; clean up the MMX state
@


1.22
log
@Replaced punpcklqdq SSE2 CPU instruction with SSE punpckldq + Removed old MMX/AMD/SSE configurations defines
@
text
@d204 1
a204 1
	sub			ecx, edi		; align source to qword
d592 1
a592 1
	sub			ecx, edi		; align source to qword
@


1.21
log
@Aw3's __stdcall calling convention
@
text
@a149 2
#if !defined(MMX) && !defined(AMD) && !defined(SSE)

d623 1
a623 1
	punpcklqdq	xmm0, xmm0
a772 703
#endif

#ifdef AMD
// MMX2 (KATMAI) / AMD Athlon & Duron
void* __stdcall memcpy_optimized(void *dest, const void *src, size_t n)
{
  __asm
  {
	cld
	mov			ebx, [n]		; number of bytes to copy
	mov			edi, [dest]		; destination
	mov			esi, [src]		; source
	push		edi
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memcpy_ic_2	; tiny? skip mmx/sse copy
	cmp			ebx, 32*1024			; don't align between 32k-64k because
	jbe			$memcpy_amd_do_align	;  it appears to be slower
	cmp			ebx, 64*1024
	jbe			$memcpy_amd_align_done
$memcpy_amd_do_align:
	mov			ecx, 8			; a trick that's faster than rep movsb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_amd_align_done
	jmp			ecx				; jump to array of movsb's

align 4
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb

$memcpy_amd_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to copy
	shr			ecx, 6			; get 64-byte block count
	jz			$memcpy_ic_2	; finish the last few bytes

	cmp			ecx, IN_CACHE_COPY/64	; too big 4 cache? use uncached copy
	jae			$memcpy_amd_uc_test

// This is small block copy that uses the MMX registers to copy 8 bytes
// at a time.  It uses the "unrolled loop" optimization, and also uses
// the software prefetch instruction to get the data into the cache.
align 16
$memcpy_amd_ic_1:			; 64-byte block copies, in-cache copy
	prefetchnta	[esi + (200*64/34+192)]		; start reading ahead

	movq		mm0, [esi+0]	; read 64 bits
	movq		mm1, [esi+8]
	movq		[edi+0], mm0	; write 64 bits
	movq		[edi+8], mm1	;    note:  the normal movq writes the
	movq		mm2, [esi+16]	;    data to cache; a cache line will be
	movq		mm3, [esi+24]	;    allocated as needed, to store the data
	movq		[edi+16], mm2
	movq		[edi+24], mm3
	movq		mm0, [esi+32]
	movq		mm1, [esi+40]
	movq		[edi+32], mm0
	movq		[edi+40], mm1
	movq		mm2, [esi+48]
	movq		mm3, [esi+56]
	movq		[edi+48], mm2
	movq		[edi+56], mm3

	add			esi, 64			; update source pointer
	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memcpy_amd_ic_1	; last 64-byte block?
$memcpy_ic_2:
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

$memcpy_amd_uc_test:
	cmp			ecx, UNCACHED_COPY/64	; big enough? use block prefetch copy
	jae			$memcpy_amd_bp_1

$memcpy_amd_64_test:			; tail end of block prefetch will jump here
	jecxz		$memcpy_ic_2	; no more 64-byte blocks left

// For larger blocks, which will spill beyond the cache, it's faster to
// use the Streaming Store instruction MOVNTQ.   This write instruction
// bypasses the cache and writes straight to main memory.  This code also
// uses the software prefetch instruction to pre-read the data.
align 16
$memcpy_amd_uc_1:				; 64-byte blocks, uncached copy
	prefetchnta	[esi + (200*64/34+192)]		; start reading ahead

	movq		mm0,[esi+0]		; read 64 bits
	add			edi,64			; update destination pointer
	movq		mm1,[esi+8]
	add			esi,64			; update source pointer
	movq		mm2,[esi-48]
	movntq		[edi-64], mm0	; write 64 bits, bypassing the cache
	movq		mm0,[esi-40]	;    note: movntq also prevents the CPU
	movntq		[edi-56], mm1	;    from READING the destination address
	movq		mm1,[esi-32]	;    into the cache, only to be over-written
	movntq		[edi-48], mm2	;    so that also helps performance
	movq		mm2,[esi-24]
	movntq		[edi-40], mm0
	movq		mm0,[esi-16]
	movntq		[edi-32], mm1
	movq		mm1,[esi-8]
	movntq		[edi-24], mm2
	movntq		[edi-16], mm0
	dec			ecx
	movntq		[edi-8], mm1
	jnz			$memcpy_amd_uc_1	; last 64-byte block?
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

// For the largest size blocks, a special technique called Block Prefetch
// can be used to accelerate the read operations.   Block Prefetch reads
// one address per cache line, for a series of cache lines, in a short loop.
// This is faster than using software prefetch, in this case.
// The technique is great for getting maximum read bandwidth,
// especially in DDR memory systems.
$memcpy_amd_bp_1:			; large blocks, block prefetch copy

	cmp			ecx, CACHEBLOCK			; big enough to run another prefetch loop?
	jl			$memcpy_amd_64_test		; no, back to regular uncached copy

	mov			eax, CACHEBLOCK / 2		; block prefetch loop, unrolled 2X
	add			esi, CACHEBLOCK * 64	; move to the top of the block
align 16
$memcpy_amd_bp_2:
	mov			edx, [esi-64]		; grab one address per cache line
	mov			edx, [esi-128]		; grab one address per cache line
	sub			esi, 128			; go reverse order
	dec			eax					; count down the cache lines
	jnz			$memcpy_amd_bp_2	; keep grabbing more lines into cache

	mov			eax, CACHEBLOCK		; now that it's in cache, do the copy
align 16
$memcpy_amd_bp_3:
	movq		mm0, [esi   ]		; read 64 bits
	movq		mm1, [esi+ 8]
	movq		mm2, [esi+16]
	movq		mm3, [esi+24]
	movq		mm4, [esi+32]
	movq		mm5, [esi+40]
	movq		mm6, [esi+48]
	movq		mm7, [esi+56]
	add			esi, 64				; update source pointer
	movntq		[edi   ], mm0		; write 64 bits, bypassing cache
	movntq		[edi+ 8], mm1		;    note: movntq also prevents the CPU
	movntq		[edi+16], mm2		;    from READING the destination address 
	movntq		[edi+24], mm3		;    into the cache, only to be over-written,
	movntq		[edi+32], mm4		;    so that also helps performance
	movntq		[edi+40], mm5
	movntq		[edi+48], mm6
	movntq		[edi+56], mm7
	add			edi, 64				; update dest pointer
	dec			eax					; count down
	jnz			$memcpy_amd_bp_3	; keep copying
	sub			ecx, CACHEBLOCK		; update the 64-byte block count
	jmp			$memcpy_amd_bp_1	; keep processing chunks

// The smallest copy uses the X86 "movsd" instruction, in an optimized
// form which is an "unrolled loop".   Then it handles the last few bytes.
align 4
	movsd
	movsd			; perform last 1-15 dword copies
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd			; perform last 1-7 dword copies
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd

$memcpy_last_few:		; dword aligned from before movsd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			movsb		; the last 1, 2, or 3 bytes

	emms				; clean up the MMX state
	sfence				; flush the write buffer
	pop			eax // [dest]	; ret value = destination pointer
    }
}

void* __stdcall memset_optimized(void *dest, int c, size_t n)
{
  __asm
  {
	cld
	mov			ebx, [n]		; number of bytes to fill
	mov			edi, [dest]		; destination
	movzx  		eax, [c]	; character
	mov			ah, al
	mov			ecx, eax
	shl			ecx, 16
	or			eax, ecx
	push		edi
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memset_ic_2	; tiny? skip optimized fill
	mov			ecx, 8			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update fill count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_amd_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memset_amd_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	jz			$memset_ic_2	; finish the last few bytes
	movd		mm0, eax
	punpckldq	mm0, mm0

align 16
$memset_amd_ic_1:

	movntq		[edi+ 0], mm0	; write 64 bits
	movntq		[edi+ 8], mm0
	movntq		[edi+16], mm0
	movntq		[edi+24], mm0
	movntq		[edi+32], mm0
	movntq		[edi+40], mm0
	movntq		[edi+48], mm0
	movntq		[edi+56], mm0

	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memset_amd_ic_1	; last 64-byte block?
$memset_ic_2:
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_last_few
	jmp			ecx				; jump to array of stosd's

align 4
	stosd
	stosd			; perform last 1-15 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd			; perform last 1-7 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd

$memset_last_few:		; dword aligned from before stosd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			stosb		; the last 1, 2, or 3 bytes

	emms				; clean up the MMX state
	sfence				; flush the write buffer
	pop			eax // [dest]	; ret value = destination pointer
    }
}
#endif AMD

#ifdef SSE
void* __stdcall memcpy_optimized(void *dest, const void *src, size_t n)
{
  __asm
  {
	cld
	mov			ebx, [n]		; number of bytes to copy
	mov			edi, [dest]		; destination
	mov			esi, [src]		; source
	push		edi
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memcpy_ic_2	; tiny? skip sse copy
	mov			ecx, 16			; a trick that's faster than rep movsb...
	sub			ecx, edi		; align source to qword
	and			ecx, 1111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_sse_align_done
	jmp			ecx				; jump to array of movsb's

align 4
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb

$memcpy_sse_align_done:		; destination is double quadword aligned
	mov			ecx, ebx		; number of bytes left to copy
	shr			ecx, 6			; get 64-byte block count
	jz			$memcpy_ic_2	; finish the last few bytes

	test		esi,1111b		; Is source address aligned?
	je			$memcpy_sse_ic_1_a

align 16
$memcpy_sse_ic_1:			; 64-byte block copies, in-cache copy
	prefetchnta	[esi + 320]		; start reading ahead

	movups		xmm0, [esi+0]		; read 128 bits
	movntps		[edi+0], xmm0		; write 128 bits
	movups		xmm1, [esi+16]
	movntps		[edi+16], xmm1
	movups		xmm2, [esi+32]
	movntps		[edi+32], xmm2
	movups		xmm3, [esi+48]
	movntps		[edi+48], xmm3

	add			esi, 64				; update source pointer
	add			edi, 64				; update destination pointer
	dec			ecx					; count down
	jnz			$memcpy_sse_ic_1	; last 64-byte block?

$memcpy_ic_2:
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

align 16
$memcpy_sse_ic_1_a:				; 64-byte block copies, in-cache copy
	prefetchnta	[esi + 320]		; start reading ahead

	movaps		xmm0, [esi+0]		; read 128 bits
	movntps		[edi+0], xmm0		; write 128 bits
	movaps		xmm1, [esi+16]
	movntps		[edi+16], xmm1
	movaps		xmm2, [esi+32]
	movntps		[edi+32], xmm2
	movaps		xmm3, [esi+48]
	movntps		[edi+48], xmm3

	add			esi, 64				; update source pointer
	add			edi, 64				; update destination pointer
	dec			ecx					; count down
	jnz			$memcpy_sse_ic_1_a	; last 64-byte block?
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

align 4
	movsd
	movsd			; perform last 1-15 dword copies
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd			; perform last 1-7 dword copies
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd

$memcpy_last_few:		; dword aligned from before movsd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			movsb		; the last 1, 2, or 3 bytes

	emms				; clean up the MMX state
	sfence				; flush the write buffer
	pop			eax // [dest]	; ret value = destination pointer
    }
}

void* __stdcall memset_optimized(void *dest, int c, size_t n)
{
  __asm
  {
	cld
	mov			ebx, [n]		; number of bytes to fill
	mov			edi, [dest]		; destination
	movzx  		eax, [c]	; character
	mov			ah, al
	mov			ecx, eax
	shl			ecx, 16
	or			eax, ecx
	push		edi
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memset_ic_2	; tiny? skip optimized fill
	mov			ecx, 16			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align source to qword
	and			ecx, 1111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_sse_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memset_sse_align_done:		; destination is double quadword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	jz			$memset_ic_2	; finish the last few bytes
	movd		xmm0, eax
	punpckldq	xmm0, xmm0
	punpcklqdq	xmm0, xmm0

align 16
$memset_sse_ic_1:

	movntps		[edi+ 0], xmm0		; write 128 bits
	movntps		[edi+16], xmm0
	movntps		[edi+32], xmm0
	movntps		[edi+48], xmm0

	add			edi, 64				; update destination pointer
	dec			ecx					; count down
	jnz			$memset_sse_ic_1	; last 64-byte block?

$memset_ic_2:
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_last_few
	jmp			ecx				; jump to array of stosd's

align 4
	stosd
	stosd			; perform last 1-15 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd			; perform last 1-7 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd

$memset_last_few:		; dword aligned from before stosd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			stosb		; the last 1, 2, or 3 bytes

	emms				; clean up the MMX state
	sfence				; flush the write buffer
	pop			eax // [dest]	; ret value = destination pointer
    }
}
#endif SSE

#ifdef MMX
void* __stdcall memcpy_optimized(void *dest, const void *src, size_t n)
{
  __asm
  {
	cld
	mov			ebx, [n]		; number of bytes to copy
	mov			edi, [dest]		; destination
	mov			esi, [src]		; source
	push		edi
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memcpy_ic_2	; tiny? skip optimized copy
	mov			ecx, 8			; a trick that's faster than rep movsb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update copy count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_mmx_align_done
	jmp			ecx				; jump to array of movsb's

align 4
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb
	movsb

$memcpy_mmx_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to copy
	shr			ecx, 6			; get 64-byte block count
	jz			$memcpy_ic_2	; finish the last few bytes

align 16
$memcpy_mmx_ic_1:			; 64-byte block copies, in-cache copy
	movq		mm0, [esi+0]	; read 64 bits
	movq		mm1, [esi+8]
	movq		[edi+0], mm0	; write 64 bits
	movq		[edi+8], mm1
	movq		mm2, [esi+16]
	movq		mm3, [esi+24]
	movq		[edi+16], mm2
	movq		[edi+24], mm3
	movq		mm0, [esi+32]
	movq		mm1, [esi+40]
	movq		[edi+32], mm0
	movq		[edi+40], mm1
	movq		mm2, [esi+48]
	movq		mm3, [esi+56]
	movq		[edi+48], mm2
	movq		[edi+56], mm3

	add			esi, 64			; update source pointer
	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memcpy_mmx_ic_1	; last 64-byte block?
$memcpy_ic_2:
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

align 4
	movsd
	movsd			; perform last 1-15 dword copies
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd			; perform last 1-7 dword copies
	movsd
	movsd
	movsd
	movsd
	movsd
	movsd

$memcpy_last_few:		; dword aligned from before movsd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			movsb		; the last 1, 2, or 3 bytes

	emms				; clean up the MMX state
	pop			eax // [dest]	; ret value = destination pointer
    }
}

void* __stdcall memset_optimized(void *dest, int c, size_t n)
{
  __asm
  {
	cld
	mov			ebx, [n]		; number of bytes to fill
	mov			edi, [dest]		; destination
	mov			al, byte ptr[c]	; character
	mov			ah, al
	push		ax
	push		ax
	pop			eax
	push		edi
	cmp			ebx, TINY_BLOCK_COPY
	jb			$memset_ic_2	; tiny? skip optimized copy
	mov			ecx, 8			; a trick that's faster than rep stosb...
	sub			ecx, edi		; align destination to qword
	and			ecx, 111b		; get the low bits
	sub			ebx, ecx		; update fill count
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_mmx_align_done
	jmp			ecx				; jump to array of stosb's

align 4
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb
	stosb

$memset_mmx_align_done:		; destination is dword aligned
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	jz			$memset_ic_2	; finish the last few bytes
	movd		mm0, eax
	punpckldq	mm0, mm0

align 16
$memset_mmx_ic_1:
	movq		[edi+ 0], mm0	; write 64 bits
	movq		[edi+ 8], mm0
	movq		[edi+16], mm0
	movq		[edi+24], mm0
	movq		[edi+32], mm0
	movq		[edi+40], mm0
	movq		[edi+48], mm0
	movq		[edi+56], mm0

	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memset_mmx_ic_1	; last 64-byte block?
$memset_ic_2:
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_last_few
	jmp			ecx				; jump to array of stosd's

align 4
	stosd
	stosd			; perform last 1-15 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd			; perform last 1-7 dword fills
	stosd
	stosd
	stosd
	stosd
	stosd
	stosd

$memset_last_few:		; dword aligned from before stosd's
	mov			ecx, ebx	; has valid low 2 bits of the byte count
	and			ecx, 11b	; the last few cows must come home
	rep			stosb		; the last 1, 2, or 3 bytes

	emms				; clean up the MMX state
	pop			eax // [dest]	; ret value = destination pointer
    }
}
#endif MMX
@


1.20
log
@Some Aw3 optimizations
@
text
@d155 1
a155 1
void* memcpy_optimized(void *dest, const void *src, size_t n)
d539 1
a539 1
void* memset_optimized(void *dest, int c, size_t n)
d779 1
a779 1
void* memcpy_optimized(void *dest, const void *src, size_t n)
d977 1
a977 1
void* memset_optimized(void *dest, int c, size_t n)
d1071 1
a1071 1
void* memcpy_optimized(void *dest, const void *src, size_t n)
d1195 1
a1195 1
void* memset_optimized(void *dest, int c, size_t n)
d1295 1
a1295 1
void* memcpy_optimized(void *dest, const void *src, size_t n)
d1388 1
a1388 1
void* memset_optimized(void *dest, int c, size_t n)
@


1.19
log
@Reverted __fastcall optimization
@
text
@d213 24
a267 1

a292 25
$memcpy_sse_ic_1_a:				; 64-byte block copies, in-cache copy

	prefetchnta	[esi + 320]		; start reading ahead

	movaps		xmm0, [esi+0]		; read 128 bits
	movntps		[edi+0], xmm0		; write 128 bits
	movaps		xmm1, [esi+16]
	movntps		[edi+16], xmm1
	movaps		xmm2, [esi+32]
	movntps		[edi+32], xmm2
	movaps		xmm3, [esi+48]
	movntps		[edi+48], xmm3

	add			esi, 64				; update source pointer
	add			edi, 64				; update destination pointer
	dec			ecx					; count down
	jnz			$memcpy_sse_ic_1_a	; last 64-byte block?
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

align 16
d307 42
a371 1

a401 44
$memcpy_amd_uc_test:
	cmp			ecx, UNCACHED_COPY/64	; big enough? use block prefetch copy
	jae			$memcpy_amd_bp_1

$memcpy_amd_64_test:
	or			ecx, ecx		; tail end of block prefetch will jump here
	jz			$memcpy_ic_2	; no more 64-byte blocks left

// For larger blocks, which will spill beyond the cache, it's faster to
// use the Streaming Store instruction MOVNTQ.   This write instruction
// bypasses the cache and writes straight to main memory.  This code also
// uses the software prefetch instruction to pre-read the data.
align 16
$memcpy_amd_uc_1:				; 64-byte blocks, uncached copy

	prefetchnta	[esi + (200*64/34+192)]		; start reading ahead

	movq		mm0,[esi+0]		; read 64 bits
	add			edi,64			; update destination pointer
	movq		mm1,[esi+8]
	add			esi,64			; update source pointer
	movq		mm2,[esi-48]
	movntq		[edi-64], mm0	; write 64 bits, bypassing the cache
	movq		mm0,[esi-40]	;    note: movntq also prevents the CPU
	movntq		[edi-56], mm1	;    from READING the destination address
	movq		mm1,[esi-32]	;    into the cache, only to be over-written
	movntq		[edi-48], mm2	;    so that also helps performance
	movq		mm2,[esi-24]
	movntq		[edi-40], mm0
	movq		mm0,[esi-16]
	movntq		[edi-32], mm1
	movq		mm1,[esi-8]
	movntq		[edi-24], mm2
	movntq		[edi-16], mm0
	dec			ecx
	movntq		[edi-8], mm1
	jnz			$memcpy_amd_uc_1	; last 64-byte block?
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's

d544 3
a546 3
	mov			ebx, [n]		; number of bytes to fill
	mov			edi, [dest]		; destination
	mov  		al, byte ptr[c]	; character
d548 3
a550 3
	push		ax
	push		ax
	pop			eax
d623 3
a625 9
	push		eax
	push		eax
	push		eax
	push		eax
	movups		xmm0,[esp]
	pop			eax
	pop			eax
	pop			eax
	pop			eax
a628 1

d670 2
a671 6

	push		eax
	push		eax
	movq		mm0,[esp]
	pop			eax
	pop			eax
d719 2
a720 5
	push		eax
	push		eax
	movq		mm0,[esp]
	pop			eax
	pop			eax
a825 1

d861 2
a862 3
$memcpy_amd_64_test:
	or			ecx, ecx		; tail end of block prefetch will jump here
	jz			$memcpy_ic_2	; no more 64-byte blocks left
a869 1

d984 1
a984 1
	mov  		al, byte ptr[c]	; character
d986 3
a988 3
	push		ax
	push		ax
	pop			eax
d1014 2
a1015 5
	push		eax
	push		eax
	movq		mm0,[esp]
	pop			eax
	pop			eax
a1117 1

a1143 1

d1202 1
a1202 1
	mov			al, byte ptr[c]	; character
d1204 3
a1206 3
	push		ax
	push		ax
	pop			eax
d1240 3
a1242 9
	push		eax
	push		eax
	push		eax
	push		eax
	movups		xmm0,[esp]
	pop			eax
	pop			eax
	pop			eax
	pop			eax
d1425 2
a1426 5
	push		eax
	push		eax
	movq		mm0,[esp]
	pop			eax
	pop			eax
@


1.18
log
@memcpy_optimized & memset_optimized now uses __fastcall - Thx Aw3 ;-)
@
text
@d155 1
a155 1
void* __fastcall memcpy_optimized(void *dest, const void *src, size_t n)
d161 3
a163 2
	mov			edi, ecx		; destination // [dest]
	mov			esi, edx		; source // [src]
d202 1
d540 1
a540 1
	mov			eax, [dest]	; ret value = destination pointer
d544 1
a544 1
void* __fastcall memset_optimized(void *dest, int c, size_t n)
d550 2
a551 2
	mov			edi, ecx		; destination // [dest]
	mov  		al, dl			; character // [c]
d556 2
d791 1
a791 1
	mov			eax, [dest]	; ret value = destination pointer
d798 1
a798 1
void* __fastcall memcpy_optimized(void *dest, const void *src, size_t n)
d804 3
a806 2
	mov			edi, ecx		; destination // [dest]
	mov			esi, edx		; source // [src]
d995 1
a995 1
	mov			eax, [dest]	; ret value = destination pointer
d999 1
a999 1
void* __fastcall memset_optimized(void *dest, int c, size_t n)
d1005 2
a1006 2
	mov			edi, ecx		; destination // [dest]
	mov			al, dl			; character // [c]
d1011 1
d1090 1
a1090 1
	mov			eax, [dest]	; ret value = destination pointer
d1096 1
a1096 1
void* __fastcall memcpy_optimized(void *dest, const void *src, size_t n)
d1102 3
a1104 2
	mov			edi, ecx		; destination // [dest]
	mov			esi, edx		; source // [src]
d1218 1
a1218 1
	mov			eax, [dest]	; ret value = destination pointer
d1222 1
a1222 1
void* __fastcall memset_optimized(void *dest, int c, size_t n)
d1228 2
a1229 2
	mov			edi, ecx		; destination // [dest]
	mov			al, dl			; character // [c]
d1234 1
d1322 1
a1322 1
	mov			eax, [dest]	; ret value = destination pointer
d1328 1
a1328 1
void* __fastcall memcpy_optimized(void *dest, const void *src, size_t n)
d1334 3
a1336 2
	mov			edi, ecx		; destination // [dest]
	mov			esi, edx		; source // [src]
d1417 1
a1417 1
	mov			eax, [dest]	; ret value = destination pointer
d1421 1
a1421 1
void* __fastcall memset_optimized(void *dest, int c, size_t n)
d1427 2
a1428 2
	mov			edi, ecx		; destination // [dest]
	mov			al, dl			; character // [c]
d1433 1
d1510 1
a1510 1
	mov			eax, [dest]	; ret value = destination pointer
@


1.17
log
@Removed FPU code + Autodetect if CPU_Type > 5
@
text
@d155 1
a155 1
void * memcpy_optimized(void *dest, const void *src, size_t n)
a159 1
proc_start:
d161 2
a162 2
	mov			edi, [dest]		; destination
	mov			esi, [src]		; source
d172 3
d187 4
a190 1
	jmp			proc_start
d542 1
a542 1
void * memset_optimized(void *dest, int c, size_t n)
a546 1
proc_start:
d548 3
a550 3
	mov			edi, [dest]		; destination
	movzx		eax, [c]		; character
	mov			ah,  al
d562 3
d577 5
a581 1
	jmp			proc_start
d794 1
a794 1
void * memcpy_optimized(void *dest, const void *src, size_t n)
d800 2
a801 2
	mov			edi, [dest]		; destination
	mov			esi, [src]		; source
d994 1
a994 1
void * memset_optimized(void *dest, int c, size_t n)
d1000 3
a1002 3
	mov			edi, [dest]		; destination
	movzx		eax, [c]		; character
	mov			ah,al
d1090 1
a1090 1
void * memcpy_optimized(void *dest, const void *src, size_t n)
d1096 2
a1097 2
	mov			edi, [dest]		; destination
	mov			esi, [src]		; source
d1215 1
a1215 1
void * memset_optimized(void *dest, int c, size_t n)
d1221 3
a1223 3
	mov			edi, [dest]		; destination
	movzx		eax, [c]		; character
	mov			ah,	 al
d1320 1
a1320 1
void * memcpy_optimized(void *dest, const void *src, size_t n)
d1326 2
a1327 2
	mov			edi, [dest]		; destination
	mov			esi, [src]		; source
d1412 1
a1412 1
void * memset_optimized(void *dest, int c, size_t n)
d1418 3
a1420 3
	mov			edi, [dest]		; destination
	movzx		eax, [c]		; character
	mov			ah,al
@


1.16
log
@Reverted FPU optimizations because they were causing 100% corruption. Now used again non optimized code.
@
text
@a78 2
//																	   //
// katsyonak: Added FPU optimized memcpy & memset - November 23, 2003 //
d83 1
a83 1
// 1 = No optimization (FPU)
d93 2
d97 2
a98 1
	;xor			eax,eax
d101 1
a101 1
	mov			eax,1 ;FPU
d114 1
a114 1
	mov			al,1 ;FPU
a181 1
//	mov			ecx,offset copy_fpu
a186 7
/*
align 16
copy_fpu:
	mov			ecx, ebx		; number of bytes left to copy
	shr			ecx, 6			; get 64-byte block count
	jz			$memcpy_ic_2	; finish the last few bytes
*/
a195 30
/*
align 16
$memcpy_fpu_ic_1:			; 64-byte block copies, in-cache copy
	fld			double ptr [esi]	; read 64 bits
	fstp		double ptr [edi]
	fld			double ptr [esi+8]
	fstp		double ptr [edi+8]
	fld			double ptr [esi+16]
	fstp		double ptr [edi+16]
	fld			double ptr [esi+24]
	fstp		double ptr [edi+24]
	fld			double ptr [esi+32]
	fstp		double ptr [edi+32]
	fld			double ptr [esi+40]
	fstp		double ptr [edi+40]
	fld			double ptr [esi+48]
	fstp		double ptr [edi+48]
	fld			double ptr [esi+56]
	fstp		double ptr [edi+56]
	add			esi, 64			; update source pointer
	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memcpy_fpu_ic_1	; last 64-byte block?
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memcpy_last_few
	jmp			ecx				; jump to array of movsd's
*/
a566 1
//	mov			ecx,offset fill_fpu
a570 10
/*
align 16
fill_fpu:
	mov			ecx, ebx		; number of bytes left to fill
	shr			ecx, 6			; get 64-byte block count
	jz			$memset_ic_2	; finish the last few bytes
	push		eax
	push		eax
	fld			double ptr [esp]
*/	
a579 11
/*
align 16
$memset_fpu_ic_1:
	fst			double ptr [edi+ 0]	; write 64 bits
	fst			double ptr [edi+ 8]
	fst			double ptr [edi+16]
	fst			double ptr [edi+24]
	fst			double ptr [edi+32]
	fst			double ptr [edi+40]
	fst			double ptr [edi+48]
	fst			double ptr [edi+56]
a580 13
	add			edi, 64			; update destination pointer
	dec			ecx				; count down
	jnz			$memset_fpu_ic_1	; last 64-byte block?
	fstp		double ptr [esp]
	pop			eax
	pop			eax
	mov			ecx, ebx		; has valid low 6 bits of the byte count
	shr			ecx, 2			; dword count
	and			ecx, 1111b		; only look at the "remainder" bits
	neg			ecx				; set up to jump into the array
	add			ecx, offset $memset_last_few
	jmp			ecx				; jump to array of stosd's
*/
@


1.15
log
@minor change
@
text
@d85 1
a85 1
// 1 = FPU
d181 2
a182 1
	mov			ecx,offset copy_fpu
d187 1
d193 11
d232 1
a232 1

d604 2
a605 1
	mov			ecx,offset fill_fpu
d609 1
a609 1

d618 11
a628 1
	
d652 1
a652 1

@


1.14
log
@AMD detection fix
@
text
@d104 1
a104 1
	jne			notamd
d106 1
a106 1
	jne			notamd
d108 1
a108 1
	jne			notamd
d110 1
a110 1
notamd:
d117 1
a117 1
	je			csse
d131 1
a131 1
csse:
a137 2
	mov			eax,1
	cpuid
@


1.13
log
@Old Pentium machines (without MMX) are now FPU optimized
@
text
@d100 1
a100 1
	mov			eax,1
d102 10
d113 1
a113 1
	mov			al,1 ;none
d116 5
a120 11
	inc			eax ;MMX
	bt			edx,25 ;SSE Feature Bit
	jnb			ret_al
;	xor			eax,eax
;	cpuid
;	cmp			ebx,68747541h ;Auth
;	jne			cpu_sse
;	cmp			edx,69746E65h ;enti
;	jne			cpu_sse
;	cmp			ecx,444D4163h ;cAMD
;	jne			cpu_sse
d124 1
a124 1
	jbe			cpu_sse
d128 8
a135 2
	jnb			cpu_sse
	mov			al,3 ;AMD / MMX2 (KATMAI)
@


1.12
log
@get_cpu_type now exists in all configurations
@
text
@d75 7
a81 5
////////////////////////////////////////////////////////////////////////////
// katsyonak: Added MMX & SSE optimized memcpy - October 8, 2003		 //
//																		//
// katsyonak: Added AMD, MMX & SSE optimized memset - October 12, 2003 //
////////////////////////////////////////////////////////////////////////
d85 1
a85 1
// 1 = No optimization
d173 1
a173 1
	mov			ecx,offset copy_rep
d179 32
a210 8
copy_rep:
	mov			ecx,ebx	
	shr			ecx,2
	rep			movsd
	mov			ecx,ebx
	and			ecx,11b
	rep			movsb
	jmp			$memcpy_exit
d583 1
a583 1
	mov			ecx,offset fill_rep
d589 31
a619 8
fill_rep:
	mov			ecx,ebx
	shr			ecx,2
	rep			stosd
	mov			ecx,ebx
	and			ecx,11b
	rep			stosb
	jmp			$memset_exit
@


1.11
log
@Added SSE2 (Pentium 4) detection
@
text
@d76 2
a77 2
// katsyonak: Added MMX & SSE optimizations - October 8, 2003  //
//															  //
a80 1
#if !defined(MMX) && !defined(AMD) && !defined(SSE)
a87 4

unsigned long memcpyProc = 0;
unsigned long memsetProc = 0;

d138 5
@


1.10
log
@minor changes
@
text
@d88 1
d131 2
d134 2
@


1.9
log
@Faster redirection for the various optimization methods on code entry
@
text
@d75 1
a75 1
//////////////////////////////////////////////////////////////////
d78 2
a79 2
// katsyonak: Added optimized memset - October 12, 2003	     //
//////////////////////////////////////////////////////////////
d99 1
a99 1
	;xor		eax,eax
d265 6
a270 1
	jmp			$memcpy_ic_2
d334 6
a339 2

	jmp			$memcpy_ic_2
d378 6
a383 1
	jmp			$memcpy_ic_2		; almost done
d480 6
a485 2
	jmp			$memcpy_ic_2

d675 6
a680 1
	jmp			$memset_ic_2
d726 6
a731 1
	jmp			$memset_ic_2
d884 6
a889 1
	jmp			$memcpy_ic_2		; almost done
a1004 1

d1154 6
a1159 1
	jmp			$memcpy_ic_2
@


1.8
log
@*** empty log message ***
@
text
@d145 1
a145 1
	mov			ecx, [n]		; number of bytes to copy
a147 1
	mov			ebx, ecx		; keep a copy of count
d149 2
a150 2
	cmp			ecx, TINY_BLOCK_COPY
	jb			$memcpy_ic_3	; tiny? skip optimized copy
d152 3
a154 4
	mov			eax,[memcpyProc]
	or 			eax,eax
	je			$memcpy_detect
	jmp			eax
d173 1
a240 1
$memcpy_ic_3:
d269 1
a269 1
	cmp			ecx, 32*1024			; don't align between 32k-64k because
d271 1
a271 1
	cmp			ecx, 64*1024
d510 1
a510 1
	mov			ecx, [n]		; number of bytes to fill
d517 6
a522 8
	mov			ebx, ecx		; keep a copy of count
	cmp			ecx, TINY_BLOCK_COPY
	jb			$memset_ic_3	; tiny? skip optimized fill

	mov			edx,[memsetProc]
	or 			edx,edx
	je			$memset_detect
	jmp			edx
d541 1
a604 1
$memset_ic_3:
d746 1
a746 1
	mov			ecx, [n]		; number of bytes to copy
d749 3
a751 4
	mov			ebx, ecx		; keep a copy of count
	cmp			ecx, TINY_BLOCK_COPY
	jb			$memcpy_ic_3	; tiny? skip mmx/sse copy
	cmp			ecx, 32*1024			; don't align between 32k-64k because
d753 1
a753 1
	cmp			ecx, 64*1024
d812 1
a812 2
	mov		ecx, ebx		; has valid low 6 bits of the byte count
$memcpy_ic_3:
d941 1
a941 1
	mov			ecx, [n]		; number of bytes to fill
d948 2
a949 3
	mov			ebx, ecx		; keep a copy of count
	cmp			ecx, TINY_BLOCK_COPY
	jb			$memset_ic_3	; tiny? skip optimized fill
a995 1
$memset_ic_3:
d1038 1
a1038 1
	mov			ecx, [n]		; number of bytes to copy
d1041 2
a1042 3
	mov			ebx, ecx		; keep a copy of count
	cmp			ecx, TINY_BLOCK_COPY
	jb			$memcpy_ic_3	; tiny? skip sse copy
a1097 1
$memcpy_ic_3:
d1158 1
a1158 1
	mov			ecx, [n]		; number of bytes to fill
d1165 2
a1166 3
	mov			ebx, ecx		; keep a copy of count
	cmp			ecx, TINY_BLOCK_COPY
	jb			$memset_ic_3	; tiny? skip optimized fill
a1220 1
$memset_ic_3:
d1263 1
a1263 1
	mov			ecx, [n]		; number of bytes to copy
d1266 2
a1267 3
	mov			ebx, ecx		; keep a copy of count
	cmp			ecx, TINY_BLOCK_COPY
	jb			$memcpy_ic_3	; tiny? skip optimized copy
a1315 1
$memcpy_ic_3:
d1355 1
a1355 1
	mov			ecx, [n]		; number of bytes to fill
d1362 2
a1363 3
	mov			ebx, ecx		; keep a copy of count
	cmp			ecx, TINY_BLOCK_COPY
	jb			$memset_ic_3	; tiny? skip optimized copy
a1407 1
$memset_ic_3:
@


1.7
log
@Squeezed out a few more CPU cycles
@
text
@d524 1
a524 1
	or 			edx,eax
@


1.6
log
@updated get_cpu_type(),
removed uchar[64*2] workaround
@
text
@d78 1
a78 1
// katsyonak: Added optimized memset - October 12, 2003		 //
d82 1
a82 1
BYTE	CPU_Type = 0;
d89 4
a92 1
void cpu_detect()
d96 4
a99 1
	xor		eax,eax
d101 3
a103 3
	or		eax,eax
	mov		eax,1
	je		cpu_done
d105 7
a111 7
	mov     al,1 ;none
	bt		edx,23 ;MMX Feature Bit
	jnb     cpu_done
	inc     eax ;MMX
	bt      edx,25 ;SSE Feature Bit
	jnb     cpu_done
;	xor		eax,eax
d113 16
a128 16
;	cmp		ebx,68747541h ;Auth
;	jne		cpu_sse
;	cmp		edx,69746E65h ;enti
;	jne		cpu_sse
;	cmp		ecx,444D4163h ;cAMD
;	jne		cpu_sse
	mov		eax,80000000h
    cpuid
    cmp		eax,80000000h
    jbe		cpu_sse
	mov		eax,80000001h
    cpuid
	bt      edx,31 ;AMD Feature Bit
	jnb		cpu_sse
	mov		al,3 ;AMD / MMX2 (KATMAI)
	jmp		cpu_done
d130 3
a132 1
	mov		al,4 ;SSE
d134 2
a135 13
	mov		CPU_Type,al
  }
}

BYTE get_cpu_type()
{
  __asm
  {
	mov		al,CPU_Type
	or		al,al
	jne		ret_al
	call	cpu_detect;
ret_al:
d145 27
a171 17
	mov		ecx, [n]		; number of bytes to copy
	mov		edi, [dest]		; destination
	mov		esi, [src]		; source
	mov		ebx, ecx		; keep a copy of count

	cmp		ecx, TINY_BLOCK_COPY
	jb		$memcpy_ic_3	; tiny? skip optimized copy

	mov     al,CPU_Type
	cmp     al,3
	ja      copy_sse
	je      copy_amd
	cmp     al,1
	ja      copy_mmx
	je		copy_rep
	call	cpu_detect;
	jmp     proc_start
d173 1
d175 6
a180 6
	shr		ecx,2
	rep		movsd
	mov		ecx,ebx
	and		ecx,11b
	rep		movsb
	jmp		$memcpy_exit
d182 1
d184 7
a190 7
	mov		ecx, 16			; a trick that's faster than rep movsb...
	sub		ecx, edi		; align source to qword
	and		ecx, 1111b		; get the low bits
	sub		ebx, ecx		; update copy count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_sse_align_done
	jmp		ecx				; jump to array of movsb's
d211 3
a213 3
	mov		ecx, ebx		; number of bytes left to copy
	shr		ecx, 6			; get 64-byte block count
	jz		$memcpy_ic_2	; finish the last few bytes
d215 2
a216 2
	test    esi,1111b		; Is the source address aligned?
	je		$memcpy_sse_ic_1_a
d224 1
a224 1
	prefetchnta [esi + 320]		; start reading ahead
d226 13
a238 13
	movups	xmm0, [esi+0]		; read 128 bits
	movntps	[edi+0], xmm0		; write 128 bits
	movups	xmm1, [esi+16]
	movntps	[edi+16], xmm1
	movups	xmm2, [esi+32]
	movntps	[edi+32], xmm2
	movups	xmm3, [esi+48]
	movntps	[edi+48], xmm3

	add		esi, 64				; update source pointer
	add		edi, 64				; update destination pointer
	dec		ecx					; count down
	jnz		$memcpy_sse_ic_1	; last 64-byte block?
d241 1
a241 1
	mov		ecx, ebx		; has valid low 6 bits of the byte count
d243 5
a247 5
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_last_few
	jmp		ecx				; jump to array of movsd's
d252 1
a252 1
	prefetchnta [esi + 320]		; start reading ahead
d254 14
a267 14
	movaps	xmm0, [esi+0]		; read 128 bits
	movntps	[edi+0], xmm0		; write 128 bits
	movaps	xmm1, [esi+16]
	movntps	[edi+16], xmm1
	movaps	xmm2, [esi+32]
	movntps	[edi+32], xmm2
	movaps	xmm3, [esi+48]
	movntps	[edi+48], xmm3

	add		esi, 64				; update source pointer
	add		edi, 64				; update destination pointer
	dec		ecx					; count down
	jnz		$memcpy_sse_ic_1_a	; last 64-byte block?
	jmp		$memcpy_ic_2
d269 1
d271 4
a274 4
	cmp		ecx, 32*1024			; don't align between 32k-64k because
	jbe		$memcpy_amd_do_align	;  it appears to be slower
	cmp		ecx, 64*1024
	jbe		$memcpy_amd_align_done
d276 7
a282 7
	mov		ecx, 8			; a trick that's faster than rep movsb...
	sub		ecx, edi		; align destination to qword
	and		ecx, 111b		; get the low bits
	sub		ebx, ecx		; update copy count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_amd_align_done
	jmp		ecx				; jump to array of movsb's
d295 3
a297 3
	mov		ecx, ebx		; number of bytes left to copy
	shr		ecx, 6			; get 64-byte block count
	jz		$memcpy_ic_2	; finish the last few bytes
d299 2
a300 2
	cmp		ecx, IN_CACHE_COPY/64	; too big 4 cache? use uncached copy
	jae		$memcpy_amd_uc_test
d308 1
a308 1
	prefetchnta [esi + (200*64/34+192)]		; start reading ahead
d310 21
a330 21
	movq	mm0, [esi+0]	; read 64 bits
	movq	mm1, [esi+8]
	movq	[edi+0], mm0	; write 64 bits
	movq	[edi+8], mm1	;    note:  the normal movq writes the
	movq	mm2, [esi+16]	;    data to cache; a cache line will be
	movq	mm3, [esi+24]	;    allocated as needed, to store the data
	movq	[edi+16], mm2
	movq	[edi+24], mm3
	movq	mm0, [esi+32]
	movq	mm1, [esi+40]
	movq	[edi+32], mm0
	movq	[edi+40], mm1
	movq	mm2, [esi+48]
	movq	mm3, [esi+56]
	movq	[edi+48], mm2
	movq	[edi+56], mm3

	add		esi, 64			; update source pointer
	add		edi, 64			; update destination pointer
	dec		ecx				; count down
	jnz		$memcpy_amd_ic_1	; last 64-byte block?
d332 1
a332 1
	jmp		$memcpy_ic_2
d335 2
a336 2
	cmp		ecx, UNCACHED_COPY/64	; big enough? use block prefetch copy
	jae		$memcpy_amd_bp_1
d339 2
a340 2
	or		ecx, ecx		; tail end of block prefetch will jump here
	jz		$memcpy_ic_2	; no more 64-byte blocks left
d349 1
a349 1
	prefetchnta [esi + (200*64/34+192)]		; start reading ahead
d351 21
a371 21
	movq	mm0,[esi+0]		; read 64 bits
	add		edi,64			; update destination pointer
	movq	mm1,[esi+8]
	add		esi,64			; update source pointer
	movq	mm2,[esi-48]
	movntq	[edi-64], mm0	; write 64 bits, bypassing the cache
	movq	mm0,[esi-40]	;    note: movntq also prevents the CPU
	movntq	[edi-56], mm1	;    from READING the destination address
	movq	mm1,[esi-32]	;    into the cache, only to be over-written
	movntq	[edi-48], mm2	;    so that also helps performance
	movq	mm2,[esi-24]
	movntq	[edi-40], mm0
	movq	mm0,[esi-16]
	movntq	[edi-32], mm1
	movq	mm1,[esi-8]
	movntq	[edi-24], mm2
	movntq	[edi-16], mm0
	dec		ecx
	movntq	[edi-8], mm1
	jnz		$memcpy_amd_uc_1	; last 64-byte block?
	jmp		$memcpy_ic_2		; almost done
d381 2
a382 2
	cmp		ecx, CACHEBLOCK			; big enough to run another prefetch loop?
	jl		$memcpy_amd_64_test		; no, back to regular uncached copy
d384 2
a385 2
	mov		eax, CACHEBLOCK / 2		; block prefetch loop, unrolled 2X
	add		esi, CACHEBLOCK * 64	; move to the top of the block
d388 5
a392 5
	mov		edx, [esi-64]		; grab one address per cache line
	mov		edx, [esi-128]		; grab one address per cache line
	sub		esi, 128			; go reverse order
	dec		eax					; count down the cache lines
	jnz		$memcpy_amd_bp_2	; keep grabbing more lines into cache
d394 1
a394 1
	mov		eax, CACHEBLOCK		; now that it's in cache, do the copy
d397 22
a418 24
	movq	mm0, [esi   ]		; read 64 bits
	movq	mm1, [esi+ 8]
	movq	mm2, [esi+16]
	movq	mm3, [esi+24]
	movq	mm4, [esi+32]
	movq	mm5, [esi+40]
	movq	mm6, [esi+48]
	movq	mm7, [esi+56]
	add		esi, 64				; update source pointer
	movntq	[edi   ], mm0		; write 64 bits, bypassing cache
	movntq	[edi+ 8], mm1		;    note: movntq also prevents the CPU
	movntq	[edi+16], mm2		;    from READING the destination address 
	movntq	[edi+24], mm3		;    into the cache, only to be over-written,
	movntq	[edi+32], mm4		;    so that also helps performance
	movntq	[edi+40], mm5
	movntq	[edi+48], mm6
	movntq	[edi+56], mm7
	add		edi, 64				; update dest pointer

	dec		eax					; count down

	jnz		$memcpy_amd_bp_3	; keep copying
	sub		ecx, CACHEBLOCK		; update the 64-byte block count
	jmp		$memcpy_amd_bp_1	; keep processing chunks
d420 1
d422 7
a428 7
	mov		ecx, 8			; a trick that's faster than rep movsb...
	sub		ecx, edi		; align destination to qword
	and		ecx, 111b		; get the low bits
	sub		ebx, ecx		; update copy count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_mmx_align_done
	jmp		ecx				; jump to array of movsb's
d441 3
a443 3
	mov		ecx, ebx		; number of bytes left to copy
	shr		ecx, 6			; get 64-byte block count
	jz		$memcpy_ic_2	; finish the last few bytes
d447 22
a468 22
	movq	mm0, [esi+0]	; read 64 bits
	movq	mm1, [esi+8]
	movq	[edi+0], mm0	; write 64 bits
	movq	[edi+8], mm1
	movq	mm2, [esi+16]
	movq	mm3, [esi+24]
	movq	[edi+16], mm2
	movq	[edi+24], mm3
	movq	mm0, [esi+32]
	movq	mm1, [esi+40]
	movq	[edi+32], mm0
	movq	[edi+40], mm1
	movq	mm2, [esi+48]
	movq	mm3, [esi+56]
	movq	[edi+48], mm2
	movq	[edi+56], mm3

	add		esi, 64			; update source pointer
	add		edi, 64			; update destination pointer
	dec		ecx				; count down
	jnz		$memcpy_mmx_ic_1	; last 64-byte block?
	jmp		$memcpy_ic_2
d491 4
a494 4
$memcpy_last_few:		; dword aligned from before movsd's
	mov		ecx, ebx	; has valid low 2 bits of the byte count
	and		ecx, 11b	; the last few cows must come home
	rep		movsb		; the last 1, 2, or 3 bytes
d496 2
a497 2
	cmp     CPU_Type,2
	jb		$memcpy_exit
d499 1
a499 1
	je		$memcpy_exit
d502 1
a502 1
	mov		eax, [dest]	; ret value = destination pointer
d512 30
a541 20
	mov		ecx, [n]		; number of bytes to fill
	mov		edi, [dest]		; destination
	movzx 	eax, [c]		; character
	mov		ah,	 al
	push	ax
	push	ax
	pop		eax
	mov		ebx, ecx		; keep a copy of count
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memset_ic_3	; tiny? skip optimized fill

	mov     dl,CPU_Type
	cmp     dl,3
	ja      fill_sse
	je      fill_amd
	cmp     dl,1
	ja      fill_mmx
	je		fill_rep
	call	cpu_detect;
	jmp     proc_start
d543 1
d545 6
a550 6
	shr		ecx,2
	rep		stosd
	mov		ecx,ebx
	and		ecx,11b
	rep		stosb
	jmp		$memset_exit
d552 1
d554 7
a560 7
	mov		ecx, 16			; a trick that's faster than rep stosb...
	sub		ecx, edi		; align source to qword
	and		ecx, 1111b		; get the low bits
	sub		ebx, ecx		; update copy count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_sse_align_done
	jmp		ecx				; jump to array of stosb's
d581 12
a592 12
	mov		ecx, ebx		; number of bytes left to fill
	shr		ecx, 6			; get 64-byte block count
	jz		$memset_ic_2	; finish the last few bytes
	push	eax
	push	eax
	push	eax
	push	eax
	movups	xmm0,[esp]
	pop		eax
	pop		eax
	pop		eax
	pop		eax
d597 8
a604 8
	movntps	[edi+ 0], xmm0		; write 128 bits
	movntps	[edi+16], xmm0
	movntps	[edi+32], xmm0
	movntps	[edi+48], xmm0

	add		edi, 64				; update destination pointer
	dec		ecx					; count down
	jnz		$memset_sse_ic_1	; last 64-byte block?
d607 1
a607 1
	mov		ecx, ebx		; has valid low 6 bits of the byte count
d609 5
a613 5
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_last_few
	jmp		ecx				; jump to array of stosd's
d615 1
d617 7
a623 7
	mov		ecx, 8			; a trick that's faster than rep stosb...
	sub		ecx, edi		; align destination to qword
	and		ecx, 111b		; get the low bits
	sub		ebx, ecx		; update fill count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_amd_align_done
	jmp		ecx				; jump to array of stosb's
d636 9
a644 9
	mov		ecx, ebx		; number of bytes left to fill
	shr		ecx, 6			; get 64-byte block count
	jz		$memset_ic_2	; finish the last few bytes

	push	eax
	push	eax
	movq	mm0,[esp]
	pop		eax
	pop		eax
d649 13
a661 13
	movntq	[edi+ 0], mm0	; write 64 bits
	movntq	[edi+ 8], mm0
	movntq	[edi+16], mm0
	movntq	[edi+24], mm0
	movntq	[edi+32], mm0
	movntq	[edi+40], mm0
	movntq	[edi+48], mm0
	movntq	[edi+56], mm0

	add		edi, 64			; update destination pointer
	dec		ecx				; count down
	jnz		$memset_amd_ic_1	; last 64-byte block?
	jmp		$memset_ic_2
d663 1
d665 7
a671 7
	mov		ecx, 8			; a trick that's faster than rep stosb...
	sub		ecx, edi		; align destination to qword
	and		ecx, 111b		; get the low bits
	sub		ebx, ecx		; update fill count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_mmx_align_done
	jmp		ecx				; jump to array of stosb's
d684 8
a691 8
	mov		ecx, ebx		; number of bytes left to fill
	shr		ecx, 6			; get 64-byte block count
	jz		$memset_ic_2	; finish the last few bytes
	push	eax
	push	eax
	movq	mm0,[esp]
	pop		eax
	pop		eax
d695 13
a707 13
	movq	[edi+ 0], mm0	; write 64 bits
	movq	[edi+ 8], mm0
	movq	[edi+16], mm0
	movq	[edi+24], mm0
	movq	[edi+32], mm0
	movq	[edi+40], mm0
	movq	[edi+48], mm0
	movq	[edi+56], mm0

	add		edi, 64			; update destination pointer
	dec		ecx				; count down
	jnz		$memset_mmx_ic_1	; last 64-byte block?
	jmp		$memset_ic_2
d728 3
a730 3
	mov		ecx, ebx	; has valid low 2 bits of the byte count
	and		ecx, 11b	; the last few cows must come home
	rep		stosb		; the last 1, 2, or 3 bytes
d732 2
a733 2
	cmp     CPU_Type,2
	jb		$memset_exit
d735 1
a735 1
	je		$memset_exit
d738 1
a738 1
	mov		eax, [dest]	; ret value = destination pointer
d750 10
a759 10
	mov		ecx, [n]		; number of bytes to copy
	mov		edi, [dest]		; destination
	mov		esi, [src]		; source
	mov		ebx, ecx		; keep a copy of count
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memcpy_ic_3	; tiny? skip mmx/sse copy
	cmp		ecx, 32*1024			; don't align between 32k-64k because
	jbe		$memcpy_amd_do_align	;  it appears to be slower
	cmp		ecx, 64*1024
	jbe		$memcpy_amd_align_done
d761 7
a767 7
	mov		ecx, 8			; a trick that's faster than rep movsb...
	sub		ecx, edi		; align destination to qword
	and		ecx, 111b		; get the low bits
	sub		ebx, ecx		; update copy count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_amd_align_done
	jmp		ecx				; jump to array of movsb's
d780 3
a782 3
	mov		ecx, ebx		; number of bytes left to copy
	shr		ecx, 6			; get 64-byte block count
	jz		$memcpy_ic_2	; finish the last few bytes
d784 2
a785 2
	cmp		ecx, IN_CACHE_COPY/64	; too big 4 cache? use uncached copy
	jae		$memcpy_amd_uc_test
d793 1
a793 1
	prefetchnta [esi + (200*64/34+192)]		; start reading ahead
d795 21
a815 21
	movq	mm0, [esi+0]	; read 64 bits
	movq	mm1, [esi+8]
	movq	[edi+0], mm0	; write 64 bits
	movq	[edi+8], mm1	;    note:  the normal movq writes the
	movq	mm2, [esi+16]	;    data to cache; a cache line will be
	movq	mm3, [esi+24]	;    allocated as needed, to store the data
	movq	[edi+16], mm2
	movq	[edi+24], mm3
	movq	mm0, [esi+32]
	movq	mm1, [esi+40]
	movq	[edi+32], mm0
	movq	[edi+40], mm1
	movq	mm2, [esi+48]
	movq	mm3, [esi+56]
	movq	[edi+48], mm2
	movq	[edi+56], mm3

	add		esi, 64			; update source pointer
	add		edi, 64			; update destination pointer
	dec		ecx				; count down
	jnz		$memcpy_amd_ic_1	; last 64-byte block?
d819 5
a823 5
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_last_few
	jmp		ecx				; jump to array of movsd's
d826 2
a827 2
	cmp		ecx, UNCACHED_COPY/64	; big enough? use block prefetch copy
	jae		$memcpy_amd_bp_1
d830 2
a831 2
	or		ecx, ecx		; tail end of block prefetch will jump here
	jz		$memcpy_ic_2	; no more 64-byte blocks left
d840 1
a840 1
	prefetchnta [esi + (200*64/34+192)]		; start reading ahead
d842 21
a862 21
	movq	mm0,[esi+0]		; read 64 bits
	add		edi,64			; update destination pointer
	movq	mm1,[esi+8]
	add		esi,64			; update source pointer
	movq	mm2,[esi-48]
	movntq	[edi-64], mm0	; write 64 bits, bypassing the cache
	movq	mm0,[esi-40]	;    note: movntq also prevents the CPU
	movntq	[edi-56], mm1	;    from READING the destination address
	movq	mm1,[esi-32]	;    into the cache, only to be over-written
	movntq	[edi-48], mm2	;    so that also helps performance
	movq	mm2,[esi-24]
	movntq	[edi-40], mm0
	movq	mm0,[esi-16]
	movntq	[edi-32], mm1
	movq	mm1,[esi-8]
	movntq	[edi-24], mm2
	movntq	[edi-16], mm0
	dec		ecx
	movntq	[edi-8], mm1
	jnz		$memcpy_amd_uc_1	; last 64-byte block?
	jmp		$memcpy_ic_2		; almost done
d872 2
a873 2
	cmp		ecx, CACHEBLOCK			; big enough to run another prefetch loop?
	jl		$memcpy_amd_64_test		; no, back to regular uncached copy
d875 2
a876 2
	mov		eax, CACHEBLOCK / 2		; block prefetch loop, unrolled 2X
	add		esi, CACHEBLOCK * 64	; move to the top of the block
d879 5
a883 5
	mov		edx, [esi-64]		; grab one address per cache line
	mov		edx, [esi-128]		; grab one address per cache line
	sub		esi, 128			; go reverse order
	dec		eax					; count down the cache lines
	jnz		$memcpy_amd_bp_2	; keep grabbing more lines into cache
d885 1
a885 1
	mov		eax, CACHEBLOCK		; now that it's in cache, do the copy
d888 22
a909 24
	movq	mm0, [esi   ]		; read 64 bits
	movq	mm1, [esi+ 8]
	movq	mm2, [esi+16]
	movq	mm3, [esi+24]
	movq	mm4, [esi+32]
	movq	mm5, [esi+40]
	movq	mm6, [esi+48]
	movq	mm7, [esi+56]
	add		esi, 64				; update source pointer
	movntq	[edi   ], mm0		; write 64 bits, bypassing cache
	movntq	[edi+ 8], mm1		;    note: movntq also prevents the CPU
	movntq	[edi+16], mm2		;    from READING the destination address 
	movntq	[edi+24], mm3		;    into the cache, only to be over-written,
	movntq	[edi+32], mm4		;    so that also helps performance
	movntq	[edi+40], mm5
	movntq	[edi+48], mm6
	movntq	[edi+56], mm7
	add		edi, 64				; update dest pointer

	dec		eax					; count down

	jnz		$memcpy_amd_bp_3	; keep copying
	sub		ecx, CACHEBLOCK		; update the 64-byte block count
	jmp		$memcpy_amd_bp_1	; keep processing chunks
d932 3
a934 3
	mov		ecx, ebx	; has valid low 2 bits of the byte count
	and		ecx, 11b	; the last few cows must come home
	rep		movsb		; the last 1, 2, or 3 bytes
d938 1
a938 1
	mov		eax, [dest]	; ret value = destination pointer
d947 17
a963 17
	mov		ecx, [n]		; number of bytes to fill
	mov		edi, [dest]		; destination
	movzx	eax, [c]		; character
	mov		ah,al
	push	ax
	push	ax
	pop		eax
	mov		ebx, ecx		; keep a copy of count
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memset_ic_3	; tiny? skip optimized fill
	mov		ecx, 8			; a trick that's faster than rep stosb...
	sub		ecx, edi		; align destination to qword
	and		ecx, 111b		; get the low bits
	sub		ebx, ecx		; update fill count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_amd_align_done
	jmp		ecx				; jump to array of stosb's
d976 9
a984 9
	mov		ecx, ebx		; number of bytes left to fill
	shr		ecx, 6			; get 64-byte block count
	jz		$memset_ic_2	; finish the last few bytes

	push	eax
	push	eax
	movq	mm0,[esp]
	pop		eax
	pop		eax
d989 12
a1000 12
	movntq	[edi+ 0], mm0	; write 64 bits
	movntq	[edi+ 8], mm0
	movntq	[edi+16], mm0
	movntq	[edi+24], mm0
	movntq	[edi+32], mm0
	movntq	[edi+40], mm0
	movntq	[edi+48], mm0
	movntq	[edi+56], mm0

	add		edi, 64			; update destination pointer
	dec		ecx				; count down
	jnz		$memset_amd_ic_1	; last 64-byte block?
d1002 1
a1002 1
	mov		ecx, ebx		; has valid low 6 bits of the byte count
d1004 5
a1008 5
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_last_few
	jmp		ecx				; jump to array of stosd's
d1012 1
a1012 1
	stosd			; perform last 1-15 dword copies
d1020 1
a1020 1
	stosd			; perform last 1-7 dword copies
d1029 3
a1031 3
	mov		ecx, ebx	; has valid low 2 bits of the byte count
	and		ecx, 11b	; the last few cows must come home
	rep		stosb		; the last 1, 2, or 3 bytes
d1035 1
a1035 1
	mov		eax, [dest]	; ret value = destination pointer
d1046 13
a1058 13
	mov		ecx, [n]		; number of bytes to copy
	mov		edi, [dest]		; destination
	mov		esi, [src]		; source
	mov		ebx, ecx		; keep a copy of count
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memcpy_ic_3	; tiny? skip sse copy
	mov		ecx, 16			; a trick that's faster than rep movsb...
	sub		ecx, edi		; align source to qword
	and		ecx, 1111b		; get the low bits
	sub		ebx, ecx		; update copy count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_sse_align_done
	jmp		ecx				; jump to array of movsb's
d1079 3
a1081 3
	mov		ecx, ebx		; number of bytes left to copy
	shr		ecx, 6			; get 64-byte block count
	jz		$memcpy_ic_2	; finish the last few bytes
d1083 2
a1084 2
	test    esi,1111b		; Is source address aligned?
	je		$memcpy_sse_ic_1_a
d1089 1
a1089 1
	prefetchnta [esi + 320]		; start reading ahead
d1091 13
a1103 13
	movups	xmm0, [esi+0]		; read 128 bits
	movntps	[edi+0], xmm0		; write 128 bits
	movups	xmm1, [esi+16]
	movntps	[edi+16], xmm1
	movups	xmm2, [esi+32]
	movntps	[edi+32], xmm2
	movups	xmm3, [esi+48]
	movntps	[edi+48], xmm3

	add		esi, 64				; update source pointer
	add		edi, 64				; update destination pointer
	dec		ecx					; count down
	jnz		$memcpy_sse_ic_1	; last 64-byte block?
d1106 1
a1106 1
	mov		ecx, ebx		; has valid low 6 bits of the byte count
d1108 5
a1112 5
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_last_few
	jmp		ecx				; jump to array of movsd's
d1117 1
a1117 1
	prefetchnta [esi + 320]		; start reading ahead
d1119 14
a1132 14
	movaps	xmm0, [esi+0]		; read 128 bits
	movntps	[edi+0], xmm0		; write 128 bits
	movaps	xmm1, [esi+16]
	movntps	[edi+16], xmm1
	movaps	xmm2, [esi+32]
	movntps	[edi+32], xmm2
	movaps	xmm3, [esi+48]
	movntps	[edi+48], xmm3

	add		esi, 64				; update source pointer
	add		edi, 64				; update destination pointer
	dec		ecx					; count down
	jnz		$memcpy_sse_ic_1_a	; last 64-byte block?
	jmp		$memcpy_ic_2
d1153 3
a1155 3
	mov		ecx, ebx	; has valid low 2 bits of the byte count
	and		ecx, 11b	; the last few cows must come home
	rep		movsb		; the last 1, 2, or 3 bytes
d1159 1
a1159 1
	mov		eax, [dest]	; ret value = destination pointer
d1168 17
a1184 17
	mov		ecx, [n]		; number of bytes to fill
	mov		edi, [dest]		; destination
	movzx 	eax, [c]		; character
	mov		ah,	 al
	push	ax
	push	ax
	pop		eax
	mov		ebx, ecx		; keep a copy of count
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memset_ic_3	; tiny? skip optimized fill
	mov		ecx, 16			; a trick that's faster than rep stosb...
	sub		ecx, edi		; align source to qword
	and		ecx, 1111b		; get the low bits
	sub		ebx, ecx		; update copy count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_sse_align_done
	jmp		ecx				; jump to array of stosb's
d1205 12
a1216 12
	mov		ecx, ebx		; number of bytes left to fill
	shr		ecx, 6			; get 64-byte block count
	jz		$memset_ic_2	; finish the last few bytes
	push	eax
	push	eax
	push	eax
	push	eax
	movups	xmm0,[esp]
	pop		eax
	pop		eax
	pop		eax
	pop		eax
d1221 8
a1228 8
	movntps	[edi+ 0], xmm0		; write 128 bits
	movntps	[edi+16], xmm0
	movntps	[edi+32], xmm0
	movntps	[edi+48], xmm0

	add		edi, 64				; update destination pointer
	dec		ecx					; count down
	jnz		$memset_sse_ic_1	; last 64-byte block?
d1231 1
a1231 1
	mov		ecx, ebx		; has valid low 6 bits of the byte count
d1233 5
a1237 5
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_last_few
	jmp		ecx				; jump to array of stosd's
d1241 1
a1241 1
	stosd			; perform last 1-15 dword copies
d1249 1
a1249 1
	stosd			; perform last 1-7 dword copies
d1258 3
a1260 3
	mov		ecx, ebx	; has valid low 2 bits of the byte count
	and		ecx, 11b	; the last few cows must come home
	rep		stosb		; the last 1, 2, or 3 bytes
d1264 1
a1264 1
	mov		eax, [dest]	; ret value = destination pointer
d1275 13
a1287 13
	mov		ecx, [n]		; number of bytes to copy
	mov		edi, [dest]		; destination
	mov		esi, [src]		; source
	mov		ebx, ecx		; keep a copy of count
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memcpy_ic_3	; tiny? skip optimized copy
	mov		ecx, 8			; a trick that's faster than rep movsb...
	sub		ecx, edi		; align destination to qword
	and		ecx, 111b		; get the low bits
	sub		ebx, ecx		; update copy count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_mmx_align_done
	jmp		ecx				; jump to array of movsb's
d1300 3
a1302 3
	mov		ecx, ebx		; number of bytes left to copy
	shr		ecx, 6			; get 64-byte block count
	jz		$memcpy_ic_2	; finish the last few bytes
d1306 21
a1326 21
	movq	mm0, [esi+0]	; read 64 bits
	movq	mm1, [esi+8]
	movq	[edi+0], mm0	; write 64 bits
	movq	[edi+8], mm1
	movq	mm2, [esi+16]
	movq	mm3, [esi+24]
	movq	[edi+16], mm2
	movq	[edi+24], mm3
	movq	mm0, [esi+32]
	movq	mm1, [esi+40]
	movq	[edi+32], mm0
	movq	[edi+40], mm1
	movq	mm2, [esi+48]
	movq	mm3, [esi+56]
	movq	[edi+48], mm2
	movq	[edi+56], mm3

	add		esi, 64			; update source pointer
	add		edi, 64			; update destination pointer
	dec		ecx				; count down
	jnz		$memcpy_mmx_ic_1	; last 64-byte block?
d1328 1
a1328 1
	mov		ecx, ebx		; has valid low 6 bits of the byte count
d1330 5
a1334 5
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_last_few
	jmp		ecx				; jump to array of movsd's
d1355 3
a1357 3
	mov		ecx, ebx	; has valid low 2 bits of the byte count
	and		ecx, 11b	; the last few cows must come home
	rep		movsb		; the last 1, 2, or 3 bytes
d1360 1
a1360 1
	mov		eax, [dest]	; ret value = destination pointer
d1369 17
a1385 17
	mov		ecx, [n]		; number of bytes to fill
	mov		edi, [dest]		; destination
	movzx	eax, [c]		; character
	mov		ah,al
	push	ax
	push	ax
	pop		eax
	mov		ebx, ecx		; keep a copy of count
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memset_ic_3	; tiny? skip optimized copy
	mov		ecx, 8			; a trick that's faster than rep stosb...
	sub		ecx, edi		; align destination to qword
	and		ecx, 111b		; get the low bits
	sub		ebx, ecx		; update fill count
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_mmx_align_done
	jmp		ecx				; jump to array of stosb's
d1398 8
a1405 8
	mov		ecx, ebx		; number of bytes left to fill
	shr		ecx, 6			; get 64-byte block count
	jz		$memset_ic_2	; finish the last few bytes
	push	eax
	push	eax
	movq	mm0,[esp]
	pop		eax
	pop		eax
d1409 12
a1420 12
	movq	[edi+ 0], mm0	; write 64 bits
	movq	[edi+ 8], mm0
	movq	[edi+16], mm0
	movq	[edi+24], mm0
	movq	[edi+32], mm0
	movq	[edi+40], mm0
	movq	[edi+48], mm0
	movq	[edi+56], mm0

	add		edi, 64			; update destination pointer
	dec		ecx				; count down
	jnz		$memset_mmx_ic_1	; last 64-byte block?
d1422 1
a1422 1
	mov		ecx, ebx		; has valid low 6 bits of the byte count
d1424 5
a1428 5
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memset_last_few
	jmp		ecx				; jump to array of stosd's
d1432 1
a1432 1
	stosd			; perform last 1-15 dword copies
d1440 1
a1440 1
	stosd			; perform last 1-7 dword copies
d1449 3
a1451 3
	mov		ecx, ebx	; has valid low 2 bits of the byte count
	and		ecx, 11b	; the last few cows must come home
	rep		stosb		; the last 1, 2, or 3 bytes
d1454 1
a1454 2
	mov		eax, [dest]	; ret value = destination pointer

@


1.5
log
@Added MMX/AMD/SSE optimized memset [katsyonak]
@
text
@d89 1
a89 3
BYTE get_cpu_type() {return CPU_Type;};

void * memcpy_optimized(void *dest, const void *src, size_t n)
d91 2
a92 20
  __asm {

	cld
proc_start:
	mov		ecx, [n]		; number of bytes to copy
	mov		edi, [dest]		; destination
	mov		esi, [src]		; source
	mov		ebx, ecx		; keep a copy of count

	cmp		ecx, TINY_BLOCK_COPY
	jb		$memcpy_ic_3	; tiny? skip optimized copy
	
	mov     al,CPU_Type
	cmp     al,3
	ja      copy_sse
	je      copy_amd
	cmp     al,1
	ja      copy_mmx
	je		copy_rep
	;check CPU
d122 3
d126 38
a163 1
    mov		CPU_Type,al
a164 3
cpu_sse:
	mov		al,4 ;SSE
	jmp		cpu_done
a493 1

d499 2
a500 2
  __asm {

d521 1
a521 32
	;check CPU
	xor		eax,eax
	cpuid
	or		eax,eax
	mov		eax,1
	je		cpu_done
	cpuid
	mov     al,1 ;none
	bt		edx,23 ;MMX Feature Bit
	jnb     cpu_done
	inc     eax ;MMX
	bt      edx,25 ;SSE Feature Bit
	jnb     cpu_done
;	xor		eax,eax
;	cpuid
;	cmp		ebx,68747541h ;Auth
;	jne		cpu_sse
;	cmp		edx,69746E65h ;enti
;	jne		cpu_sse
;	cmp		ecx,444D4163h ;cAMD
;	jne		cpu_sse
	mov		eax,80000000h
    cpuid
    cmp		eax,80000000h
    jbe		cpu_sse
	mov		eax,80000001h
    cpuid
	bt      edx,31 ;AMD Feature Bit
	jnb		cpu_sse
	mov		al,3 ;AMD / MMX2 (KATMAI)
cpu_done:
    mov		CPU_Type,al
a522 3
cpu_sse:
	mov		al,4 ;SSE
	jmp		cpu_done
d724 2
a725 2
  __asm {

a917 1

d923 2
a924 2
  __asm {

a1014 1

d1022 2
a1023 2
  __asm {

a1138 1

d1144 2
a1145 2
  __asm {

a1243 1

d1251 2
a1252 2
  __asm {

a1339 1

d1345 2
a1346 2
  __asm {

@


1.4
log
@Katsyonak update
@
text
@d75 5
a79 1
// katsyonak: Added MMX & SSE optimizations - October 8, 2003
d89 2
d189 1
a189 1
	test    esi,1111b
d198 1
a198 1
	prefetchnta [esi + 320]		; start reading ahead ;(200*64/34+192)
d202 3
a204 3
	movups	xmm1, [esi+16]		;    note:  the normal movups writes the
	movntps	[edi+16], xmm1		;    data to cache; a cache line will be
	movups	xmm2, [esi+32]		;    allocated as needed, to store the data
d226 1
a226 1
	prefetchnta [esi + 320]		; start reading ahead ;(200*64/34+192)
d230 3
a232 3
	movaps	xmm1, [esi+16]		;    note:  the normal movups writes the
	movntps	[edi+16], xmm1		;    data to cache; a cache line will be
	movaps	xmm2, [esi+32]		;    allocated as needed, to store the data
a395 2
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memcpy_ic_3	; tiny? skip mmx/sse copy
d420 1
a420 1
$memcpy_mmx_ic_1:			; 64-byte block copies, in-cache copy
a467 1
	jz		$memcpy_final	; no more, let's leave
a469 1
$memcpy_final: 
d480 256
a930 1
	jz		$memcpy_final	; no more, let's leave
d933 98
a1030 1
$memcpy_final: 
d1082 1
a1082 1
	test    esi,1111b
a1084 3
// This is small block copy that uses the SSE registers to copy 16 bytes
// at a time.  It uses the "unrolled loop" optimization, and also uses
// the software prefetch instruction to get the data into the cache.
d1088 1
a1088 1
	prefetchnta [esi + 320]		; start reading ahead ;(200*64/34+192)
d1092 3
a1094 3
	movups	xmm1, [esi+16]		;    note:  the normal movups writes the
	movntps	[edi+16], xmm1		;    data to cache; a cache line will be
	movups	xmm2, [esi+32]		;    allocated as needed, to store the data
d1116 1
a1116 1
	prefetchnta [esi + 320]		; start reading ahead ;(200*64/34+192)
d1120 3
a1122 3
	movaps	xmm1, [esi+16]		;    note:  the normal movups writes the
	movntps	[edi+16], xmm1		;    data to cache; a cache line will be
	movaps	xmm2, [esi+32]		;    allocated as needed, to store the data
a1132 2
// The smallest copy uses the X86 "movsd" instruction, in an optimized
// form which is an "unrolled loop".   Then it handles the last few bytes.
a1153 1
	jz		$memcpy_final	; no more, let's leave
d1156 106
a1261 1
$memcpy_final: 
d1281 1
a1281 1
	jb		$memcpy_ic_3	; tiny? skip mmx copy
a1336 2
// The smallest copy uses the X86 "movsd" instruction, in an optimized
// form which is an "unrolled loop".   Then it handles the last few bytes.
a1357 1
	jz		$memcpy_final	; no more, let's leave
d1360 95
a1454 1
$memcpy_final: 
a1460 1

@


1.3
log
@Katsyonak changes for memcpy optimization (should be tested in all configurations)
Release (auto optimize), AMD, SSE and MMX release profiles.
@
text
@d78 6
a83 1
BYTE CPU_Type = 0; // 0 = CPU check not performed yet / 1 = None / 2 = MMX / 3 = MMX2 (KATMAI) / 4 = SSE
d95 3
d102 1
a102 1
	je      copy_mmx
d104 1
a104 1
	ja      copy_mmx1
d133 1
a133 1
	bt      edx,31
d135 1
a135 1
	mov		al,3 ;MMX2 (KATMAI)
d140 1
a140 1
	mov		al,4
a151 7
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memcpy_ic_3	; tiny? skip mmx/sse copy
	cmp		ecx, 32*1024			; don't align between 32k-64k because
	jbe		$memcpy_sse_do_align	;  it appears to be slower
	cmp		ecx, 64*1024
	jbe		$memcpy_sse_align_done
$memcpy_sse_do_align:
d183 2
a184 2
	cmp		ecx, IN_CACHE_COPY/64	; too big 4 cache? use uncached copy
	jae		$memcpy_sse_uc_test
d192 1
a192 1
	prefetchnta [esi + (200*64/34+192)]		; start reading ahead
d195 1
a195 1
	movaps	[edi+0], xmm0		; write 128 bits
d197 1
a197 1
	movaps	[edi+16], xmm1		;    data to cache; a cache line will be
d199 1
a199 1
	movaps	[edi+32], xmm2
d201 1
a201 1
	movaps	[edi+48], xmm3
a216 12
$memcpy_sse_uc_test:
	cmp		ecx, UNCACHED_COPY/64	; big enough? use block prefetch copy
	jae		$memcpy_sse_bp_1

$memcpy_sse_64_test:
	or		ecx, ecx		; tail end of block prefetch will jump here
	jz		$memcpy_ic_2	; no more 64-byte blocks left

// For larger blocks, which will spill beyond the cache, it's faster to
// use the Streaming Store instruction movntps.   This write instruction
// bypasses the cache and writes straight to main memory.  This code also
// uses the software prefetch instruction to pre-read the data.
d218 1
a218 26
$memcpy_sse_uc_1:			; 64-byte blocks, uncached copy

	prefetchnta [esi + (200*64/34+192)]		; start reading ahead

	movups	xmm0,[esi+0] 	; read 128 bits
	add		edi,64			; update destination pointer
	movups	xmm1,[esi+16] ;16
	add		esi,64			; update source pointer
	movups	xmm2,[esi-32]
	movntps	[edi-64], xmm0 	; write 128 bits, bypassing the cache
	movups	xmm3,[esi-16]	;    note: movntps also prevents the CPU
	movntps	[edi-48], xmm1 	;    from READING the destination address
	movntps	[edi-32], xmm2	;    into the cache, only to be over-written
	dec		ecx				;    so that also helps performance
	movntps	[edi-16], xmm3	;    into the cache, only to be over-written
	jnz		$memcpy_sse_uc_1	; last 64-byte block?

	jmp		$memcpy_ic_2		; almost done

// For the largest size blocks, a special technique called Block Prefetch
// can be used to accelerate the read operations.   Block Prefetch reads
// one address per cache line, for a series of cache lines, in a short loop.
// This is faster than using software prefetch, in this case.
// The technique is great for getting maximum read bandwidth,
// especially in DDR memory systems.
$memcpy_sse_bp_1:			; large blocks, block prefetch copy
d220 1
a220 2
	cmp		ecx, CACHEBLOCK			; big enough to run another prefetch loop?
	jl		$memcpy_sse_64_test			; no, back to regular uncached copy
d222 8
a229 9
	mov		eax, CACHEBLOCK / 2		; block prefetch loop, unrolled 2X
	add		esi, CACHEBLOCK * 64	; move to the top of the block
align 16
$memcpy_sse_bp_2:
	mov		edx, [esi-64]		; grab one address per cache line
	mov		edx, [esi-128]		; grab one address per cache line
	sub		esi, 128			; go reverse order
	dec		eax					; count down the cache lines
	jnz		$memcpy_sse_bp_2	; keep grabbing more lines into cache
a230 7
	mov		eax, CACHEBLOCK		; now that it's in cache, do the copy
align 16
$memcpy_sse_bp_3:
	movups	xmm0, [esi   ]		; read 128 bits
	movups	xmm1, [esi+16]
	movups	xmm2, [esi+32]
	movups	xmm3, [esi+48]
d232 4
a235 7
	movntps	[edi   ], xmm0		; write 128 bits, bypassing cache
	movntps	[edi+16], xmm1		;    note: movntps also prevents the CPU from READING the 
	movntps	[edi+32], xmm2		;    destination addressinto the cache, only to be over-written,
	movntps	[edi+48], xmm3		;    so that also helps performance
	add		edi, 64				; update dest pointer

	dec		eax					; count down
d237 1
a237 7
	jnz		$memcpy_sse_bp_3	; keep copying
	sub		ecx, CACHEBLOCK		; update the 64-byte block count
	jmp		$memcpy_sse_bp_1	; keep processing chunks

copy_mmx:
	cmp		ecx, TINY_BLOCK_COPY
	jb		$memcpy_ic_3	; tiny? skip mmx/sse copy
d239 1
a239 1
	jbe		$memcpy_mmx_do_align	;  it appears to be slower
d241 2
a242 2
	jbe		$memcpy_mmx_align_done
$memcpy_mmx_do_align:
d248 1
a248 1
	add		ecx, offset $memcpy_mmx_align_done
d261 1
a261 1
$memcpy_mmx_align_done:		; destination is dword aligned
d267 1
a267 1
	jae		$memcpy_mmx_uc_test
d273 1
a273 1
$memcpy_mmx_ic_1:			; 64-byte block copies, in-cache copy
d297 1
a297 1
	jnz		$memcpy_mmx_ic_1	; last 64-byte block?
d301 1
a301 1
$memcpy_mmx_uc_test:
d303 1
a303 1
	jae		$memcpy_mmx_bp_1
d305 1
a305 1
$memcpy_mmx_64_test:
d314 1
a314 1
$memcpy_mmx_uc_1:				; 64-byte blocks, uncached copy
d337 1
a337 1
	jnz		$memcpy_mmx_uc_1	; last 64-byte block?
d346 1
a346 1
$memcpy_mmx_bp_1:			; large blocks, block prefetch copy
d349 1
a349 1
	jl		$memcpy_mmx_64_test		; no, back to regular uncached copy
d354 1
a354 1
$memcpy_mmx_bp_2:
d359 1
a359 1
	jnz		$memcpy_mmx_bp_2	; keep grabbing more lines into cache
d363 1
a363 1
$memcpy_mmx_bp_3:
d385 1
a385 1
	jnz		$memcpy_mmx_bp_3	; keep copying
d387 1
a387 1
	jmp		$memcpy_mmx_bp_1	; keep processing chunks
d389 1
a389 1
copy_mmx1:
d397 1
a397 1
	add		ecx, offset $memcpy_mmx1_align_done
d410 1
a410 1
$memcpy_mmx1_align_done:	; destination is dword aligned
d416 1
a416 1
$memcpy_mmx1_ic_1:			; 64-byte block copies, in-cache copy
d437 1
a437 1
	jnz		$memcpy_mmx1_ic_1	; last 64-byte block?
d468 2
a470 1
	cmp     CPU_Type,2
d494 1
a494 1
	jbe		$memcpy_mmx_do_align	;  it appears to be slower
d496 2
a497 2
	jbe		$memcpy_mmx_align_done
$memcpy_mmx_do_align:
d503 1
a503 1
	add		ecx, offset $memcpy_mmx_align_done
d516 1
a516 1
$memcpy_mmx_align_done:		; destination is dword aligned
d522 1
a522 1
	jae		$memcpy_mmx_uc_test
d528 1
a528 1
$memcpy_mmx_ic_1:			; 64-byte block copies, in-cache copy
d552 1
a552 1
	jnz		$memcpy_mmx_ic_1	; last 64-byte block?
d562 1
a562 1
$memcpy_mmx_uc_test:
d564 1
a564 1
	jae		$memcpy_mmx_bp_1
d566 1
a566 1
$memcpy_mmx_64_test:
d575 1
a575 1
$memcpy_mmx_uc_1:				; 64-byte blocks, uncached copy
d598 1
a598 1
	jnz		$memcpy_mmx_uc_1	; last 64-byte block?
d607 1
a607 1
$memcpy_mmx_bp_1:			; large blocks, block prefetch copy
d610 1
a610 1
	jl		$memcpy_mmx_64_test		; no, back to regular uncached copy
d615 1
a615 1
$memcpy_mmx_bp_2:
d620 1
a620 1
	jnz		$memcpy_mmx_bp_2	; keep grabbing more lines into cache
d624 1
a624 1
$memcpy_mmx_bp_3:
d646 1
a646 1
	jnz		$memcpy_mmx_bp_3	; keep copying
d648 1
a648 1
	jmp		$memcpy_mmx_bp_1	; keep processing chunks
a696 5
	cmp		ecx, 32*1024			; don't align between 32k-64k because
	jbe		$memcpy_sse_do_align	;  it appears to be slower
	cmp		ecx, 64*1024
	jbe		$memcpy_sse_align_done
$memcpy_sse_do_align:
d728 2
a729 2
	cmp		ecx, IN_CACHE_COPY/64	; too big 4 cache? use uncached copy
	jae		$memcpy_sse_uc_test
d737 1
a737 1
	prefetchnta [esi + (200*64/34+192)]		; start reading ahead
d740 1
a740 1
	movaps	[edi+0], xmm0		; write 128 bits
d742 1
a742 1
	movaps	[edi+16], xmm1		;    data to cache; a cache line will be
d744 1
a744 1
	movaps	[edi+32], xmm2
d746 1
a746 1
	movaps	[edi+48], xmm3
a761 12
$memcpy_sse_uc_test:
	cmp		ecx, UNCACHED_COPY/64	; big enough? use block prefetch copy
	jae		$memcpy_sse_bp_1

$memcpy_sse_64_test:
	or		ecx, ecx		; tail end of block prefetch will jump here
	jz		$memcpy_ic_2	; no more 64-byte blocks left

// For larger blocks, which will spill beyond the cache, it's faster to
// use the Streaming Store instruction movntps.   This write instruction
// bypasses the cache and writes straight to main memory.  This code also
// uses the software prefetch instruction to pre-read the data.
d763 1
a763 26
$memcpy_sse_uc_1:			; 64-byte blocks, uncached copy

	prefetchnta [esi + (200*64/34+192)]		; start reading ahead

	movups	xmm0,[esi+0] 	; read 128 bits
	add		edi,64			; update destination pointer
	movups	xmm1,[esi+16] ;16
	add		esi,64			; update source pointer
	movups	xmm2,[esi-32]
	movntps	[edi-64], xmm0 	; write 128 bits, bypassing the cache
	movups	xmm3,[esi-16]	;    note: movntps also prevents the CPU
	movntps	[edi-48], xmm1 	;    from READING the destination address
	movntps	[edi-32], xmm2	;    into the cache, only to be over-written
	dec		ecx				;    so that also helps performance
	movntps	[edi-16], xmm3	;    into the cache, only to be over-written
	jnz		$memcpy_sse_uc_1	; last 64-byte block?

	jmp		$memcpy_ic_2		; almost done

// For the largest size blocks, a special technique called Block Prefetch
// can be used to accelerate the read operations.   Block Prefetch reads
// one address per cache line, for a series of cache lines, in a short loop.
// This is faster than using software prefetch, in this case.
// The technique is great for getting maximum read bandwidth,
// especially in DDR memory systems.
$memcpy_sse_bp_1:			; large blocks, block prefetch copy
d765 1
a765 2
	cmp		ecx, CACHEBLOCK			; big enough to run another prefetch loop?
	jl		$memcpy_sse_64_test			; no, back to regular uncached copy
d767 8
a774 9
	mov		eax, CACHEBLOCK / 2		; block prefetch loop, unrolled 2X
	add		esi, CACHEBLOCK * 64	; move to the top of the block
align 16
$memcpy_sse_bp_2:
	mov		edx, [esi-64]		; grab one address per cache line
	mov		edx, [esi-128]		; grab one address per cache line
	sub		esi, 128			; go reverse order
	dec		eax					; count down the cache lines
	jnz		$memcpy_sse_bp_2	; keep grabbing more lines into cache
a775 7
	mov		eax, CACHEBLOCK		; now that it's in cache, do the copy
align 16
$memcpy_sse_bp_3:
	movups	xmm0, [esi   ]		; read 128 bits
	movups	xmm1, [esi+16]
	movups	xmm2, [esi+32]
	movups	xmm3, [esi+48]
d777 4
a780 11
	movntps	[edi   ], xmm0		; write 128 bits, bypassing cache
	movntps	[edi+16], xmm1		;    note: movntps also prevents the CPU from READING the 
	movntps	[edi+32], xmm2		;    destination addressinto the cache, only to be over-written,
	movntps	[edi+48], xmm3		;    so that also helps performance
	add		edi, 64				; update dest pointer

	dec		eax					; count down

	jnz		$memcpy_sse_bp_3	; keep copying
	sub		ecx, CACHEBLOCK		; update the 64-byte block count
	jmp		$memcpy_sse_bp_1	; keep processing chunks
d834 1
a834 1
	add		ecx, offset $memcpy_mmx1_align_done
d847 1
a847 1
$memcpy_mmx1_align_done:	; destination is dword aligned
d853 1
a853 1
$memcpy_mmx1_ic_1:			; 64-byte block copies, in-cache copy
d874 1
a874 1
	jnz		$memcpy_mmx1_ic_1	; last 64-byte block?
@


1.2
log
@Formatting, comments, and name changes.
Removed #includes for "memcpy_amd.h".
@
text
@d75 1
a75 1
// katsyonak: Added CPU check for SSE/MMX optimization or none - October 8, 2003
d77 2
a78 1
BYTE CPU_Type = 0; // 0 = CPU check not performed yet / 1 = None / 2 = MMX / 3 = SSE
d84 2
a89 2

	cld
d92 1
a92 2
cpu_check:
	cmp     al,2
d95 3
a97 2
	or      al,al
	jne     copy_rep
d105 23
a127 7
	mov     al,3
	test    edx,02000000h ;SSE Feature Bit
	jne     cpu_done
	dec     eax
	test    edx,00800000h ;MMX Feature Bit
	jne     cpu_done
	dec     eax
d130 4
a133 1
	jmp     cpu_check
d443 52
d523 2
d531 498
@


1.1
log
@AMD optimized memcpy()
@
text
@d75 5
a79 1
void * memcpy_amd(void *dest, const void *src, size_t n)
d89 35
d125 74
a198 1
	jb		$memcpy_ic_3	; tiny? skip mmx copy
d200 74
a273 2
	cmp		ecx, 32*1024		; don't align between 32k-64k because
	jbe		$memcpy_do_align	;  it appears to be slower
d275 2
a276 2
	jbe		$memcpy_align_done
$memcpy_do_align:
d282 1
a282 1
	add		ecx, offset $memcpy_align_done
d295 1
a295 1
$memcpy_align_done:			; destination is dword aligned
d301 1
a301 1
	jae		$memcpy_uc_test
d307 1
a307 1
$memcpy_ic_1:			; 64-byte block copies, in-cache copy
d331 1
a331 1
	jnz		$memcpy_ic_1	; last 64-byte block?
d333 1
a333 8
$memcpy_ic_2:
	mov		ecx, ebx		; has valid low 6 bits of the byte count
$memcpy_ic_3:
	shr		ecx, 2			; dword count
	and		ecx, 1111b		; only look at the "remainder" bits
	neg		ecx				; set up to jump into the array
	add		ecx, offset $memcpy_last_few
	jmp		ecx				; jump to array of movsd's
d335 1
a335 1
$memcpy_uc_test:
d337 1
a337 1
	jae		$memcpy_bp_1
d339 1
a339 1
$memcpy_64_test:
d348 1
a348 1
$memcpy_uc_1:				; 64-byte blocks, uncached copy
d371 1
a371 2
	jnz		$memcpy_uc_1	; last 64-byte block?

d380 1
a380 1
$memcpy_bp_1:			; large blocks, block prefetch copy
d383 1
a383 1
	jl		$memcpy_64_test			; no, back to regular uncached copy
d388 1
a388 1
$memcpy_bp_2:
d393 1
a393 1
	jnz		$memcpy_bp_2		; keep grabbing more lines into cache
d397 1
a397 1
$memcpy_bp_3:
d419 1
a419 1
	jnz		$memcpy_bp_3		; keep copying
d421 1
a421 1
	jmp		$memcpy_bp_1		; keep processing chunks
d452 1
@

